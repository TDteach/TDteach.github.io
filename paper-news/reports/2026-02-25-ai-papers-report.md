# AI & AI Safety Daily Paper Report
## 2026-02-25

**Generated at**: 2026-02-25 23:23:52
**Paper count**: 22

---

## 1. Research Problems

### Hallucination/Factuality
- LLM-based scholarly question-answering tools are increasingly used to help researchers synthesize and edit text based on large bodies of scientific literature, but they can produce subtle and high-stakes errors such as unsupported claims and omissions. The paper argues that common provenance mechanisms—especially coarse source citations—are insufficiently granular for rigorous scholarly verification, and may even be inaccurate or hallucinated, leaving domain experts without actionable ways to validate specific assertions (p. 2, Section 1). This creates a trust and reliance calibration problem: researchers need to know not just which documents were used, but which specific claims in an LLM answer are supported by which evidence, and what relevant information was omitted. The authors further motivate that scholarly work is time-pressured and cognitively demanding, so even when users become skeptical, they may not change behavior if verification is too burdensome (p. 2, Section 1). The core problem is therefore twofold: (1) designing provenance that is fine-grained and aligned with scholarly argumentation structures (claims supported by evidence), and (2) understanding whether such provenance actually changes users’ trust perceptions and reliance behaviors in realistic scholarly editing workflows. Addressing this matters because ungrounded or subtly wrong scholarly outputs can propagate errors through academic discourse, and because interface interventions that merely change attitudes (trust) may fail to change behavior (reliance) under real constraints (p. 2, Section 1; p. 16–17, Section 7.1).
- Large Vision-Language Models (LVLMs) can generate fluent but incorrect responses that are not supported by the input image (hallucinations), which creates safety and reliability risks in real-world deployment (p. 1, Abstract; p. 1, Introduction). A common way to improve reliability is self-evaluation: the model estimates whether its own output is correct using internal signals, avoiding costly external judges that can themselves hallucinate (p. 1, Introduction). However, existing self-evaluation methods largely originate from language-only settings and tend to rely on language priors (e.g., common textual associations) rather than verifying whether the answer is grounded in the visual evidence (p. 2, Introduction; p. 3, Section 4.1). The paper highlights a failure mode where standard uncertainty-based scores can assign low uncertainty to hallucinated answers when the image contradicts common expectations, because the model remains confident due to linguistic regularities (p. 2, Introduction; p. 1, Fig. 1 caption; p. 3, Section 4.1). This mismatch is especially problematic in counterfactual or adversarially constructed settings where visual grounding is essential. The core problem is therefore to design a training-free self-evaluation score for LVLMs that explicitly measures how much the model’s prediction depends on visual evidence, so that confidence reflects grounding rather than fluency (p. 2, Introduction; p. 3, Section 4.1).

### Other
- Not enough information.
- LLM-based agents are increasingly expected to solve real-world tasks through command-line interfaces (CLIs), where success requires multi-step interaction with a live environment (e.g., installing dependencies, editing files, running tests, debugging). While benchmarks like Terminal-Bench evaluate these capabilities in containerized environments with programmatic verification, the training data strategies that enable strong terminal agents are often undisclosed, leaving practitioners without clear guidance on how to build effective supervised fine-tuning (SFT) datasets for terminal interaction (p. 2, Introduction; p. 1, Abstract). The paper frames two practical bottlenecks for scaling agentic terminal data: (1) scarcity of foundational resources such as diverse prompts, dependency files, and pre-configured environments; and (2) the logistical and computational complexity of collecting trajectories, since real human terminal traces are hard to capture and synthetic rollouts are expensive due to per-task environment instantiation and multi-turn interaction (p. 2, Introduction). Existing approaches either focus on agent scaffolding or on wrapping existing datasets into terminal interfaces via adapters, but adapters may inherit structural assumptions from non-interactive datasets and multi-agent generation pipelines can be costly to scale (p. 2, Introduction; p. 3, Related Work). The core problem is therefore how to engineer scalable, diverse, and high-quality training data—covering both broad foundational skills and terminal-specific operational skills—so that relatively modest-sized open models can substantially improve terminal performance on realistic benchmarks (p. 4, Section 4; p. 8, Section 5.2).
- Test-time training (TTT) has evolved from a technique for adapting models under distribution shift into a sequence-modeling architectural primitive that can serve as an alternative to softmax attention, with favorable efficiency properties such as linear-time compute and constant memory during autoregressive inference (p. 1, Section 1). A prominent variant, TTT with key–value binding (TTT-KVB), is widely interpreted as an online meta-learning / fast-weights mechanism that “memorizes” a key–value mapping in an inner loop and then “retrieves” information by applying the updated fast-weight function to queries (p. 1, Section 1; p. 3, Section 3). This storage-and-retrieval interpretation has driven increasingly complex architectural choices (e.g., sophisticated optimizers, normalization, deeper inner-loop networks) intended to improve memorization fidelity (p. 1, Section 1).  The paper argues that this prevailing interpretation is inconsistent with observed behavior in trained TTT models. The authors identify several empirical “paradoxes” that contradict what one would expect if the inner loop were truly performing useful memorization and query-based retrieval: downstream performance can worsen as inner-loop fitting improves; gradient ascent can replace gradient descent without harming performance; queries and keys can be distributionally mismatched; and replacing queries with keys can have negligible effect (p. 1, Section 1; p. 3–4, Section 4). These anomalies matter because they suggest that current design intuitions for TTT may be misguided, potentially leading to unnecessary complexity and missed opportunities for simplification and acceleration. The core problem is therefore to correctly characterize what computation TTT-KVB layers implement, and to use that characterization to explain the paradoxes and enable more principled, efficient designs (p. 2, Section 1; p. 4–6, Section 5; p. 7–8, Section 6).
- Most logical-reasoning evaluations for large language models (LLMs) focus on convergent reasoning: producing one correct proof or final label for a given set of premises. The paper argues this misses an important real-world property of reasoning problems: many conclusions can be supported by multiple distinct, valid derivations, and effective reasoners should be able to explore and enumerate alternative proof routes rather than committing early to a single path. The authors formalize this as a multi-path logical reasoning task: given premises P and a goal hypothesis G, the model should enumerate all distinct logically valid derivation paths that entail G, not merely decide entailment. They define each derivation path as a minimal support set S of premises such that S entails G and no strict subset of S entails G. Building a benchmark for this task is challenging because it requires (i) scalable construction with exhaustive ground truth over all minimal proofs, (ii) reliable evaluation of open-ended, multi-solution natural-language proof generation, and (iii) metrics that distinguish mere correctness from genuine exploration/coverage of the proof space. Without such a benchmark and evaluation, models that appear strong under standard accuracy can still systematically miss alternative valid proofs, masking a key limitation in flexible reasoning and search behavior as reasoning depth and branching increase.
- Not enough information.
- Chain-of-Thought (CoT) prompting is widely used to elicit multi-step reasoning in large language models, improving accuracy on reasoning-intensive tasks, but it often produces long rationales that increase inference latency and cost in production settings (p. 1, Section 1). This creates a practical tradeoff between high-accuracy, high-compute inference and faster zero-shot inference that may lack reasoning depth (p. 1, Section 1). A common industry response is knowledge distillation via fine-tuning a smaller “student” model to mimic a larger “teacher,” including approaches that distill step-by-step reasoning traces (p. 1, Section 1; p. 2, Section 2.2). However, the paper argues that parametric distillation/fine-tuning introduces operational burdens: it can fail to transfer true reasoning when only teacher text outputs are available, requires retraining when the teacher or domain logic changes, does not scale to many use-cases, and often needs large curated datasets to avoid issues like catastrophic forgetting (p. 1–2, Section 1). The core problem is therefore how to transfer reasoning capability to smaller or cheaper models while preserving interpretability and avoiding the latency overhead of CoT and the maintenance overhead of fine-tuning. The paper targets reasoning-intensive classification settings (e.g., legal NLI, compliance-like decision boundaries) where decisions should be verifiable and where runtime generation of long rationales is undesirable (p. 2, Section 1; p. 3, Section 3).
- The paper reports and documents the performance of Aletheia, a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge, which consists of ten research-level mathematics questions proposed to assess AI capabilities on problems arising from professional mathematicians’ work. A central issue the paper addresses is how to interpret and operationalize “autonomous” research-level problem solving in a setting where the challenge rules are not fully formalized. The FirstProof FAQ emphasizes that an AI “has answered” a question if it can produce, autonomously, a proof meeting the rigor and scholarship standards of the mathematics literature, without relying on human input for mathematical ideas, and with appropriate citations. The authors argue that ambiguity remains around what counts as autonomy (e.g., whether post-hoc clarification requests are allowed, or whether humans may select the best attempt among multiple runs), and that these choices can materially affect reported performance. Against this backdrop, the paper’s goal is to provide a transparent account of an evaluation pipeline that (i) ensures no human intervention during solution generation, (ii) applies a fixed verification/extraction prompt to standardize outputs toward publishable proofs, and (iii) uses expert mathematician assessments to judge correctness under an explicitly stated interpretation (“publishable after minor revisions”). The broader motivation is to understand current AI reliability and capability for research mathematics, including the agent’s tendency to self-filter by returning no solution when it cannot produce one within constraints.
- Not enough information.
- The paper investigates a key empirical bottleneck in the “Diligent Learner” framework: whether large language models (LLMs) can sustain a non-vanishing per-step success probability (denoted γ) as multi-step reasoning depth increases. In the Diligent Learner view, reasoning is modeled as validator-guided search over semantic steps; test-time search can scale to solve bounded-depth problems only if the model’s proposal distribution keeps enough probability mass on “good” next steps at each depth (p. 5, Section 3.1). While the theory provides guarantees when γ stays bounded away from zero, it remains unclear whether γ collapses on out-of-distribution (OOD) logical inference tasks that require integrating accumulated state with new evidence at every step (p. 1–2, Introduction).  The authors argue existing reasoning benchmarks are inadequate for measuring γ because they often score only final answers, allow multiple valid intermediate trajectories, or permit shortcut strategies such as pattern matching or memorization of benchmark regularities (p. 2, Introduction; p. 4–5). To address this, they design a benchmark where each step has a unique correct continuation and where ignoring either the revealed history (prefix) or the fresh step-specific evidence is intended to be information-theoretically ineffective (p. 2; p. 4–6). The core problem matters because if γ degrades sharply with depth on such OOD tasks, then simply increasing test-time compute/search would not yield robust long-horizon reasoning; conversely, if γ can be stabilized (e.g., via tools), it supports the feasibility of the Diligent Learner pathway to highly capable agents (p. 5; p. 14–16).
- Training Transformer models on very long sequences is constrained by GPU memory, especially the activation memory required during training. Context parallelism (also called sequence parallelism) addresses this by sharding the sequence across multiple devices, enabling longer contexts than a single accelerator can handle. However, dominant context-parallel approaches such as Ring Attention and DeepSpeed-Ulysses primarily focus on scaling computation/communication over the context dimension, while the per-device activation memory still grows linearly with sequence length and can become the bottleneck at multi-million-token contexts (p. 1, Section 1). The paper argues that, beyond roughly multi-million token lengths, even with distributed sharding, the intermediate tensors in self-attention—particularly the full-head QKV tensors and the communication buffers used by all-to-all—create an “activation memory barrier” that prevents further scaling (p. 1, Section 1; p. 5, Section 3.2). Existing methods that push context further (e.g., FPDT or activation offloading) can reduce GPU memory pressure but often incur throughput penalties due to CPU involvement and data transfers (p. 1, Abstract; p. 3, Section 2.1; p. 7, Section 5.2). The core problem is therefore to extend maximum trainable context length under context parallelism by reducing attention activation memory—especially the intermediate tensors and communication buffers—while maintaining training throughput comparable to high-performance baselines like Ulysses (p. 1, Abstract; p. 2, Section 1).
- Selective state space models (SSMs) such as Mamba are increasingly used as backbones for large language models, particularly for long-context workloads, because they process sequences in a streaming manner with linear-time sequence processing and a compact recurrent state rather than quadratic self-attention. However, when deployed for inference, SSM serving performance is often constrained by single-GPU limits: GPU memory capacity (for weights and runtime buffers), bandwidth, and latency. As model sizes and supported context lengths grow, single-GPU execution becomes insufficient, making multi-GPU inference necessary. While tensor parallelism (TP) is a standard approach for scaling Transformer inference by sharding large matrix multiplications, applying TP to selective SSM blocks is non-trivial. The SSM mixer combines large projections with locality-sensitive operations (depthwise convolution and a sequence-wise recurrent state update/scan) that rely on contiguous per-channel layouts and minimal synchronization. Naively reusing Transformer TP sharding patterns can fragment packed intermediate tensors and force additional AllGather/AllReduce collectives at multiple points in the mixer, placing synchronization on the critical path and harming throughput. Additionally, autoregressive serving has distinct prefill and decode phases; without careful caching, SSMs may redundantly reprocess the prompt context for each generated token, inflating latency. The paper targets these systems challenges by designing a communication-efficient TP approach tailored to selective SSM inference, aiming to improve throughput, reduce latency (including time-to-first-token), and enable longer prompts by distributing memory footprint across multiple GPUs.
- Late-interaction multi-vector retrieval (e.g., ColBERT-style MaxSim) has become a strong paradigm for retrieval across modalities such as text, visual documents, and video. However, its storage and query-time computation scale linearly with document length because each document token (or frame/audio token) contributes a vector to the index. This linear dependence becomes prohibitive for multimodal corpora where a single item can contain thousands of tokens, leading to extremely large indices (the paper gives a back-of-the-envelope example for video indexing scale) and high scoring costs (p. 1, Introduction; p. 2, Introduction). The paper also argues that this cost is often wasteful: in multimodal late interaction, many indexed vectors are underutilized during retrieval, so full-length indices provide diminishing returns relative to their expense (p. 2, Introduction; p. 8–9, Section 6.5).   The core problem is therefore: how to compress multi-vector document representations in a query-agnostic way (since indexing happens before queries are known) while preserving the fine-grained discriminative information needed for late interaction to separate relevant documents from hard negatives (p. 3, Section 3.1; p. 2, “Attention-based Compression”). The paper frames this as learning or constructing a mapping from an arbitrary-length document to a fixed-size set of m vectors (a constant vector budget), so that both index size and query-time scoring are bounded and tunable to compute/storage constraints, without giving up the benefits of multi-vector matching (p. 3, Section 3.1).
- Not enough information.
- The paper addresses a specific barrier faced by lay users when they first encounter an unfamiliar, complex Knowledge Graph (KG): they often cannot even begin exploring because they lack the orientation needed to form a starting point. The authors argue that existing KG usability and exploratory search discussions treat early-stage difficulties in a fragmented way and typically assume that users can start “somewhere” (e.g., by issuing a query, selecting a facet, or navigating from an initial entity). In contrast, at first contact, lay users may not know what the KG contains, how its knowledge is structured, or how to express questions in a formal query language, which can prevent any meaningful initial interaction.  They theorize this as the Initial Exploration Problem (IEP), a temporally bounded “pre-goal” state occurring at first contact with an unfamiliar KG. The IEP matters because KGs are increasingly used across domains (e.g., medicine, encyclopedic knowledge bases, digital humanities), but their benefits may not reach intended communities if non-expert users cannot access them. The paper claims that many interfaces embed epistemic assumptions (e.g., that users can articulate a query or interpret schema elements) that do not hold at first contact, creating a structural gap in the design space: a lack of mechanisms that reveal KG scope without requiring users to already know what to ask or how the ontology is organized. The authors propose the IEP as a theoretical lens to evaluate KG interfaces and motivate entry-point scaffolding for initial exploration (p. 2, Introduction; p. 6–8, Section 4).

### Privacy/Security
- LLM-driven agentic systems are increasingly used as trusted copilots for consequential tasks (e.g., email handling, hiring, healthcare documentation). This trust creates a distinct security risk the paper calls Agent-Mediated Deception (AMD): rather than merely attacking the model, an adversary compromises parts of the agentic workflow (e.g., perception inputs, memory, or tools) so the agent faithfully executes a corrupted mission and then persuades the human user to accept the manipulated outcome. The core problem is not only whether agents can be compromised (agent-centric security), but whether humans can detect and respond appropriately when their delegated agent becomes a deceptive insider threat. The paper argues AMD differs from traditional online deception (e.g., phishing/misinformation) because users are in an “active delegation” mindset and the deception is mediated through a trusted assistant, potentially bypassing normal skepticism toward external messages (p. 1–3, Sections I–II.D).  The work targets three research questions: (RQ1) how susceptible users are to AMD, (RQ2) which cognitive factors predict susceptibility, and (RQ3) how to design human-centric defenses (p. 1, Section I). The motivation is that human factors in this setting are underexplored, and relying solely on technical defenses is risky because agents operate in open, untrusted environments where perfect technical safeguards are unlikely (p. 12, Section VI). Therefore, measuring real user behavior under realistic agent workflows is necessary to understand the true security posture of agentic systems and to inform defenses that meaningfully support human oversight.

### Robustness
- LLM-based agents are increasingly deployed for complex, tool-using tasks (e.g., web browsing, code execution, data analysis), but existing benchmarks largely fail to measure whether these systems can perform realistic information synthesis: gathering evidence from multiple sources and then integrating it to infer insights beyond simple fact lookup. The paper argues that many benchmarks over-emphasize shallow retrieval, rely on single well-known sources (e.g., Wikipedia), and are often English-centric, which limits their ability to evaluate agents in globally distributed, heterogeneous information ecosystems. In real decision-making settings (e.g., policy, travel, socio-economic analysis), answering questions often requires navigating multiple websites, extracting both structured and unstructured data, performing computations, and producing structured outputs that can be verified. The authors therefore target the evaluation gap for “deep” synthesis tasks that are time-consuming and multi-step, where answers are not directly retrievable verbatim and require analysis (e.g., trend detection, ranking, correlation). They also emphasize the need for verifiable, stable answers to enable reproducible evaluation, while still ensuring robustness against memorization and contamination. The core problem is thus to create an evaluation benchmark that (1) reflects real-world multi-source synthesis, (2) is verifiable and stable over time, (3) is diverse across regions and domains, and (4) is difficult enough to expose current agents’ limitations such as hallucinations, navigation failures, and brittle reasoning over large information spaces.
- Pass@k is a standard metric for verifiable LLM tasks (e.g., math/code) where a model can sample k independent solutions and a verifier checks correctness; success is declared if any sample passes (p. 3, Section 2). This metric has motivated post-training methods that directly optimize pass@k via policy gradients, aiming to match the inference-time procedure (p. 1, Section 1; p. 4, Section 2). However, multiple prior works report a recurring and practically problematic trade-off: optimizing pass@k can increase pass@k while decreasing pass@1 (single-shot success) (p. 1, Section 1). The paper argues this matters because pass@1 often remains an operational constraint due to latency/cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback (p. 2, “Why this degradation matters.”). The core problem is therefore to explain when and why inference-aware optimization for multi-sample success (pass@k) can harm single-sample performance (pass@1), despite the fact that per-prompt pass@k gradients are positive scalings of per-prompt pass@1 gradients (p. 7, Section 4.1; p. 4, Section 2). The authors frame the open question as understanding the mechanism behind this degradation and providing conditions under which it occurs, so that future inference-aware fine-tuning can avoid sacrificing pass@1 while improving multi-attempt performance (p. 2, “Open question.”; p. 15, Section 7).
- LLM-based agents operate in interactive environments by producing multi-step action trajectories rather than single responses. Training such agents with reinforcement learning often relies on sparse outcome rewards (e.g., success/failure), which provide limited guidance about which intermediate decisions helped or hurt. Prior work has introduced step-level credit assignment to distribute reward across steps, but the paper argues that a key learning signal is still underused: the LLM’s intrinsic uncertainty. The authors motivate uncertainty as important because it reflects confidence at each decision point and can indicate when exploration is needed (high uncertainty) versus when behavior should be reinforced for convergence (low uncertainty). Additionally, uncertainty can provide informative feedback even when a trajectory fails, potentially allowing learning from otherwise uninformative failures. The core problem addressed is therefore how to design rewards for RL-trained LLM agents that (i) incorporate uncertainty as a dense, internal signal aligned with confidence, and (ii) reshape rewards so that failed trajectories still contribute useful learning signals, improving exploration efficiency and training stability in interactive benchmarks such as ALFWorld and WebShop. (p. 1 Abstract; p. 2 Introduction; p. 5–6 Section 3.3)
- Multi-agent imitation learning (MA-IL) seeks to learn policies from demonstrations of expert interactions in Markov games. While prior offline IL theory often provides guarantees in terms of performance/suboptimality (i.e., how well the learned joint policy matches the expert’s value), this does not address strategic robustness: a learned joint policy can be highly exploitable if one agent can gain by deviating unilaterally. The paper targets this gap by studying how far an offline-imitation-learned product policy can be from a Nash equilibrium, measured via the Nash gap (maximal unilateral improvement over players). The core problem is that common offline MA-IL objectives/diagnostics—behavioral cloning (BC) error and occupancy-measure matching error—do not, in general Markov games, reliably predict exploitability. The authors ask when one can derive (i) consistent bounds (Nash gap goes to zero as imitation error goes to zero) and (ii) tractable bounds (computable efficiently, enabling monitoring during training). They show that even in idealized regimes (exact measure matching) and even when the game is known, there are fundamental barriers: exact matching can still yield highly exploitable policies, and computing tight lower bounds on achievable exploitability given an approximation error can be computationally intractable. They then identify structural assumptions—strategic dominance and a continuity property of best responses—that enable consistent, tractable upper bounds linking BC error to Nash gap. (Definitions of Nash gap and BC/measure-matching errors: p. 4, Section 3.3; motivation and contributions: p. 1–2, Introduction.)
- The paper addresses sample selection bias and train–serving discrepancy in industrial recommender-system pre-ranking. In cascade recommenders, pre-ranking must efficiently score thousands of recalled items and pass only a small subset to downstream ranking. However, pre-rankers are typically trained only on exposed interactions (items actually shown to users with observed clicks/purchases), while at serving time they must score the entire recalled set, including many unexposed candidates that were retrieved upstream but never displayed and thus have no feedback (p. 1, Introduction; p. 3, Section 3.1). This mismatch induces a distribution shift and severe sample selection bias, because exposure is itself biased (e.g., toward popular items), which can degrade generalization and suppress long-tail or novel content discovery (p. 1, Introduction; p. 3, Section 3.1). Existing mitigation strategies—treating unexposed items as negatives via negative sampling, collecting randomized exploration traffic, distilling pseudo-labels from downstream rankers, or adversarial domain adaptation—either create false negatives, are costly/risky, or propagate exposure bias from teachers trained on exposed logs (p. 1–2, Introduction; p. 2, Section 2.1). The core problem is therefore how to obtain reliable supervision for unexposed candidates so that the pre-ranker’s training distribution better matches the online serving space, improving robustness and diversity without adding online latency (p. 2, Introduction; p. 5, Section 3.5).

---

## 2. Methods & Approaches

### Hallucination/Factuality
- PaperTrail is a system and interface that operationalizes argument-grounded provenance by decomposing both source papers and LLM answers into discrete claims and evidence, then matching them to expose supported claims, unsupported claims, and omissions (p. 2, Section 1; p. 4–8, Section 3). The backend uses a three-stage pipeline (Fig. 2): (Stage 1) offline paper-level claim extraction using an LLM (Gemini 2.5 Pro) prompted to extract atomic, verifiable, faithful, decontextualized, declarative claims from each paragraph; evidence candidates are retrieved via sentence-transformer similarity with a cosine threshold of 0.75 and expanded with surrounding sentences for readability (p. 4–5, Section 3.1.1). (Stage 2) real-time answer generation (document-grounded QA in the study setup) followed by LLM-based extraction of answer claims and supporting evidence into a structured JSON schema; spans are mapped back to text using sentence tokenization and character-position matching for highlighting (p. 5, Section 3.1.2). (Stage 3) real-time matching: relevant paper claims/evidence are selected via a RAG-style process combining SPECTER embedding similarity retrieval with LLM selection; then an LLM performs claim-to-claim semantic equivalence matching; answer evidence is “verified” with cosine similarity and a permissive threshold (<0.55) flags potentially unsupported evidence (p. 7, Section 3.1.3). The frontend is a coordinated three-panel interface showing the task editor, chat, and provenance panel with claim coverage and included/omitted claim cards (p. 7–8, Section 3.2; Fig. 3). The authors evaluate (a) offline claim extraction quality on SciClaimHunt and BioClaimDetect using SPECTER similarity matching at τ=0.9 to compute precision/recall/F1 (p. 8–10, Section 4), and (b) a within-subjects user study (N=26) comparing PaperTrail to a citation-style baseline across two scholarly editing tasks, measuring trust (TXAI), reliance (normalized token-level Levenshtein similarity), confidence, and secondary workload/usability metrics (p. 10–13, Sections 5–6; Table 1).
- VAUQ is a training-free self-evaluation framework that combines predictive uncertainty with an explicit measure of visual information utilization (p. 2, Introduction; p. 4, Section 4.2; Fig. 2). First, it computes length-normalized predictive entropy H(y|v,t) over the generated sequence given image tokens v and text prompt t (p. 4, Definition 4.1 footnote). Second, it introduces an Image-Information Score (IS) that measures how much the image reduces uncertainty by comparing entropy with and without visual tokens: IS_blank = H(y|∅,t) − H(y|v,t) (p. 4, Eq. 2). To reduce sensitivity to spurious background correlations, VAUQ uses an unsupervised core-region masking strategy based on the model’s visual attention (p. 4, Section 4.2). It aggregates attention from generated tokens to image tokens across heads and a contiguous layer range (ls to le): Attn(vi) = Σ_{l=ls..le} Σ_h Σ_j A^{(l,h)}(y_j, v_i) (p. 5, Eq. 3). It then selects the top K% most-attended image patches v_top (p. 5, Eq. 4) and masks them to form v_masked, computing a core-masked information score IScore = H(y|v_masked,t) − H(y|v,t) (p. 5, Eq. 5). Finally, VAUQ defines a scoring function s_VAUQ(x,y) = H(y|v,t) − α·IScore, where α weights the penalty for weak reliance on core visual evidence (p. 5, Eq. 6). Implementation uses greedy decoding (max length 128) and, for efficiency, masks attention weights (attention knockout) rather than raw pixels when computing IScore (p. 12, Appendix A).

### Other
- Not enough information.
- The paper proposes a coarse-to-fine data engineering framework, Terminal-Task-Gen, that combines (i) dataset adaptation and (ii) synthetic task generation, followed by trajectory generation and post-processing to produce an SFT corpus (Terminal-Corpus) (p. 1, Fig. 1 caption; p. 4, Section 4). For dataset adaptation, the authors select prompt datasets spanning math, code, and software engineering (SWE), then map each prompt into a Terminal-Bench-style instruction using the Terminus 2 system prompt template and dataset-specific instruction suffixes; SWE prompts also instantiate referenced code files into the environment, but these adapted tasks do not include test cases (p. 5, Section 4.1.2; p. 18, Figs. 8–10). For synthetic task generation, they use two approaches: seed-based generation (LLM transforms structured seed problems into self-contained terminal tasks, generating input files and pytest tests; reference solutions, if present, are used only to derive test expectations and are not exposed to the agent) and skill-based generation (LLM composes tasks from a curated taxonomy of primitive skills across 9 domains, typically combining 3–5 skills per task) (p. 5–7, Sections 4.2.1–4.2.2). Both methods output tasks with prompts, pytest tests (with weights), files, and domain-specific Docker environments; they avoid generating oracle solutions and emphasize solution isolation to prevent leakage (p. 7, Section 4.2.3). To scale generation, they use 9 pre-built domain Docker images rather than per-task Dockerfiles (p. 7, Section 4.2.3). DeepSeek-V3.2 is used as the teacher model for generating synthetic tasks and trajectories via the Terminus 2 agent scaffold (p. 7, Section 4.3; p. 4, Section 3.2). Post-processing includes decontamination via 14-gram overlap removal against Terminal-Bench 2.0 test samples and quality filters (identity leaks, Chinese characters), and they ablate additional trajectory filtering choices (p. 8, Section 4.4; p. 9–10, Sections 5.4–5.5). Models (Nemotron-Terminal) are trained by SFT starting from Qwen3 8B/14B/32B with specified hyperparameters (e.g., max sequence length 32,768) and evaluated on Terminal-Bench 2.0 using the Terminus 2 reference agent (p. 8, Section 5.1; p. 4, Section 3.1).
- The paper combines empirical probes with analytical reformulation. Empirically, it tests predictions of the memorization view by manipulating inner-loop behavior at inference time and measuring downstream task metrics. Examples include varying the number of inner-loop gradient steps (showing inner loss improves while task performance degrades; Fig. 1, p. 3–4, Section 4.1), swapping gradient descent for gradient ascent (Table 1, p. 4, Section 4.2), visualizing query/key distributions via t-SNE to assess distributional overlap (Fig. 2, p. 4, Section 4.3), and replacing queries with keys when computing outputs (Table 1, p. 4, Section 4.4).  Analytically, the authors “unroll” the inner-loop updates and prove that a broad class of TTT-KVB architectures can be rewritten as a learned linear-attention-like operator. The key technical condition is that the inner-loop function has a linear, bias-free final layer f(x)=ϕ(x;Θ)W (p. 5, Theorem 5.1). After one gradient step on a key input, the updated output on a query can be expressed in a linear attention form o = q̂ (S0 + k̂^T v̂), where q̂, k̂, v̂ are induced by the network features and the loss gradient (p. 5, Theorem 5.1). Repeating over a sequence yields an “extended” linear attention accumulation over history (p. 5, Theorem 5.2). The framework is extended to gradient descent with momentum, yielding momentum-weighted effective values (p. 5, Theorem 5.3). They then instantiate the reduction for specific TTT variants: LaCT (with SwiGLU inner-loop MLP, Frobenius inner-product loss, per-token learning rates, momentum, and gradient orthogonalization) is rewritten in linear-attention-like form (p. 6, Section 5.3; Appendix E), and ViTTT’s GLU and depthwise convolution fast-weight components are similarly expressed as linear-attention-like mechanisms (p. 6–7, Section 5.4; Appendix F–G).  Finally, they use this perspective to define an ablation trajectory that progressively removes or simplifies inner-loop components (e.g., update only last layer, remove weight normalization, reduce MLP depth, remove per-token learning rates, remove momentum, remove gradient orthogonalization) and evaluate performance/throughput, including a parallel formulation enabled when associativity holds (p. 7–8, Section 6; Table 2; Appendix H–I).
- LogicGraph is constructed with an automated neuro-symbolic pipeline that guarantees exhaustive multi-path ground truth. First, the authors generate a symbolic “Logic DAG” via backward (bottom-up) construction from a sampled conclusion: for each node they sample one of seven fundamental argument forms (e.g., Modus Ponens, Modus Tollens, Hypothetical Syllogism, Disjunctive Syllogism, Constructive Dilemma, Reductio ad Absurdum, Disjunction Elimination) and generate parent premises, recursively expanding to a target depth. To create multiple paths, they build an initial chain, then select intermediate conclusions and expand them upward again, producing a DAG with shared inference nodes; they avoid unintended extra paths by assigning fresh atomic identifiers unless nodes are explicitly shared. Second, they semantically instantiate the symbolic DAG into Prover9 expressions and then into natural-language narratives: they sample from 32 abstract entity types and use an LLM (Deepseek-V3.2-Exp) to map abstract symbols to domain-specific predicates and verbalize them while preserving logical relations. Third, they filter/verify each instance with Prover9 using stepwise entailment checks for each inference edge, global derivability checks for connectivity from premises to goal, and contextual satisfiability checks to avoid contradictions.  For evaluation of model-generated multi-path proofs, they propose a reference-free neuro-symbolic evaluator: an LLM extracts and formalizes each natural-language step into Prover9-style logic, then Prover9 verifies (i) local validity of each step from cited premises and (ii) global validity that the final goal follows from the subset of premises used. They also provide a two-dimensional error taxonomy (semantic comprehension vs logical execution) and define convergent metrics (success rate, precision, shortest-path finding rate) and divergent metrics (solution recall/diversity, family recall/versatility, and originality based on inverse frequency across models).
- Not enough information.
- The paper introduces Prompt-Level Distillation (PLD), a supervised, non-parametric framework that compiles a teacher model’s reasoning into a student model’s system prompt rather than updating model weights (p. 2, Section 1; p. 3, Section 3). PLD has four phases (Fig. 1; p. 3, Section 3). Phase 1 (Supervised Instruction Extraction) uses a labeled training set T={(xi, yi)} and prompts a reasoning-optimized teacher to (i) produce a chain-of-thought rationale supporting the gold label and (ii) immediately abstract that rationale into a generalized “executable” natural-language rule, yielding an augmented dataset D={(xi, yi, Ii)} (p. 3, Section 3.1; Appendix A.1). Phase 2 (Clustering Logic Synthesis) embeds the extracted micro-instructions using an embedding model and clusters them with DBSCAN using cosine distance; outliers/noise are discarded, and the teacher (or same model) synthesizes each cluster into a unified instruction (p. 4, Section 3.2; Appendix C.1–C.2). Phase 3 (Closed-Loop Conflict Resolution) iteratively evaluates the student with the current consolidated instructions on training/validation data, isolates failure cases, and uses a conflict-resolution model (same as teacher) to refine instructions using both failures and successful examples; the loop repeats until validation error converges (p. 4–5, Section 3.3). Phase 4 deploys the refined consolidated instruction set as the student’s system prompt for zero-shot inference, aiming to avoid generating intermediate reasoning tokens at runtime (p. 5, Section 3.4). The experimental setup uses Gemini 3 Flash/Pro as teacher/conflict-resolution models and evaluates students including Gemma-3 4B and Gemini 2 Flash on Contract-NLI and StereoSet (p. 5, Section 4; p. 5, Section 4.3).
- The authors run Aletheia by prompting it with the FirstProof problem statements copied verbatim from the official LaTeX source, with no modifications. Generated outputs are then passed through a pre-determined “verification and extraction prompt” (Appendix A) executed via Gemini 3 Deep Think, which instructs the model to independently verify the candidate solution, issue a verdict ([CORRECT]/[WRONG]/[FIXABLE]), and, if fixable, produce a complete corrected LaTeX proof. This prompt is also used to elicit LaTeX directly, avoiding manual reformatting. The study uses two variants of the agent differing by base model: Aletheia A (same base model as Gemini 3 Deep Think as of Feb 2026) and Aletheia B (Jan 2026 Gemini base model referenced in prior work). They perform a “best-of-2” selection across these two agents, designating one preferred solution per problem (a human choice) for reporting. For evaluation, they solicit independent feedback from at least two academic mathematicians per problem, increasing the number of experts when confidence is lower; results are summarized as counts of experts rating a solution correct. They also report qualitative evaluation notes for specific problems (e.g., misinterpretation, critical flaw, or incompleteness) and discuss an “inference cost” analysis, plotting per-problem inference cost relative to a prior baseline (Erdős-1051), noting especially high cost for Problem 7. The paper additionally includes raw prompts/outputs for solved problems in Appendix C and records pre-deadline internal evaluations in Appendix B.
- Not enough information.
- The paper introduces a stepwise Boolean function reconstruction task over GF(2), where the target function is represented in Algebraic Normal Form (ANF) as an XOR of monomials. Inputs are split into address bits a and payload bits v, and each ANF term has the form tj(a,v)=aj·Mj(v), where Mj(v) is a degree-(d−1) monomial over a subset Sj of payload variables (p. 6, Section 4.1, Eq. 3). An instance is an ordered sequence of supports (S1,…,Sn), making the next correct term unique at each step (p. 6, Section 4.1).  At step g, the model receives (i) the prefix Pg=(t1,…,tg) and (ii) a fresh labeled dataset Sg sampled from a step-g oracle; it must output the next monomial tg+1. Step-success is defined as γg = Pr[ t̂ = tg+1 ] under the model’s stochastic policy conditioned on (Pg,Sg) (p. 6, Section 4.1, Eq. 4). To prevent shortcuts, the authors design a “statistical obfuscation” sampling oracle: labels decompose into a prefix-dependent mask XOR a next-term signal, so data-only solvers see near-random labels unless they condition on the prefix (p. 8, Section 4.2, Eq. 6; Lemma 4.3). They also sample supports i.i.d. so the prefix provides no information about the next support, defeating history-only prediction (p. 7, Section 4.2, Lemma 4.1). Payloads are drawn from a fixed Hamming-weight sphere and a weight w* is chosen to make monomial firing probability near 1/2 to reduce label bias (p. 7–8, Section 4.2, Lemma 4.2).  For evaluation, they convert the task into a single-turn prompt containing metadata, the prefix terms, and a table of 32 labeled observations; the prompt specifies the active address variable and constrains the search to payload indices (p. 9, Section 5.1). Validation is efficient because there is exactly one valid continuation; correctness is checked by parsing the predicted indices and comparing sets (p. 10, Section 5.2). They also provide a polynomial-time decoder for diligent solvers via residualization and intersection over positive examples (Appendix B, Eq. 7–8; Theorem B.1).
- UPipe is introduced as an “untied” variant of DeepSpeed-Ulysses that reduces attention activation memory by chunking attention execution along the attention-head dimension (p. 1, Abstract; p. 5, Section 3.3). The method builds on DS-Ulysses’ approach of sharding the sequence across C devices and using all-to-all to reshard QKV from sequence-sharded to head-sharded layouts so each device can attend over the full sequence for a subset of heads (p. 4, Section 3.1). UPipe’s key change is to avoid materializing full-head QKV and full-size all-to-all buffers at once: it processes only U heads per stage (U < H), repeating for H/U stages, and reuses the same HBM buffers across stages so peak intermediate memory scales with U rather than H (p. 5, Sections 3.2–3.3; Fig. 3). The paper provides a memory analysis showing DS-Ulysses intermediate tensors are proportional to H, while UPipe replaces H with U, yielding a tunable runtime–memory tradeoff; the smallest valid U is U=C (U divisible by C) (p. 5–6, Sections 3.3–3.4). To remain compatible with Grouped Query Attention (GQA), UPipe introduces a scheduling strategy that communicates as many unique KV heads as possible early and then reuses them across subsequent stages by selecting queries “out of order,” reducing redundant KV communication (p. 6–7, Section 4.1; Fig. 4). Implementation-wise, experiments integrate UPipe into TorchTitan, use FlashAttention-3 kernels, and combine with other memory-saving components (tiled FFN/RMSNorm, fused linear+cross-entropy loss, and activation checkpointing with CPU offloading) to enable multi-million-token training (p. 6, Section 4; p. 7, Section 5.1).
- The paper proposes a tensor-parallel inference design specialized for Mamba-style selective SSM mixer blocks. First, it introduces an SSM cache to avoid redundant prompt reprocessing across prefill and decode: the cache stores (a) the compact per-layer SSM recurrent state after processing the prompt and (b) a short convolution history for the causal depthwise convolution; under TP, the cache is sharded by channels so each GPU stores only the cache entries for the channels it owns, keeping cache reads/writes GPU-local during decoding (p. 4, Section IV-A; p. 5, Fig. 2 caption). Second, it uses an “intelligent channel-wise splitting” (a channel splitter) that shards weights and activations along channels so each GPU can execute locality-sensitive operators (channel-separable depthwise Conv1D and the SSM scan/update) on contiguous local channel shards without communication in those paths (p. 5, Section IV-B). This design reduces required communication collectives from four per block under naive sharding to two AllReduces per block: one after the SSM-parameter projection and one at the residual-stream boundary (p. 5, Section IV-B). Third, it explicitly handles packed SSM parameter tensors: instead of naively slicing the packed SSM_parameters tensor (which can split logical fields like Δ, B, C), it unpacks logical fields and applies TP-aware placement—sharding Δ with channels while ensuring other quantities needed for local state updates are locally available (token-dependent B and C produced locally; per-channel A and D replicated/stored as needed) so the fused SSM kernel can run on contiguous local shards without extra collectives (p. 6, Section IV-C). Finally, it optionally quantizes the remaining AllReduce payloads by quantizing communicated tensors from FP32 to FP16 for transfer/reduction and dequantizing afterward, reducing bandwidth demand while leaving SSM/convolution kernels unchanged (p. 6, Section IV-D). The design is stated to extend to Mamba-2, Falcon-Mamba, and Zamba, adapting only to implementation-specific packed layouts and using standard TP for Zamba’s attention component where applicable (p. 6, Section IV-E).
- The paper uses ColBERT-style late interaction with MaxSim scoring between a query token sequence Q and a document token sequence C (p. 3, “Late Interaction”). It studies query-agnostic compression of document representations into a fixed budget m vectors, via four approaches (p. 2, Introduction; p. 3–5, Section 4–5).  (1) SeqResize: a parameterized method that encodes the full document with a bidirectional transformer, pads/truncates to a fixed length n0, then applies a 2-layer MLP that operates over the sequence dimension to project from length n0 to length m (p. 3–4, Section 4.1).  (2) MemTok: a parameterized method that appends m learnable “memory tokens” to the document token sequence, runs the transformer encoder, and uses the final hidden states of the memory tokens as the compressed representation (p. 4, Section 4.2).  (3) H-Pool: a non-parametric method that computes pairwise cosine distances and performs agglomerative hierarchical clustering with Ward linkage, merging until the desired number of clusters remains; each cluster is represented by the mean of its member vectors, with an option to keep m′ protected tokens (p. 4, Section 4.3).  (4) AGC (Attention-Guided Clustering): the proposed method. It appends trainable “universal query” tokens to the document, uses their last-layer attention to compute token saliency scores (averaged over heads and universal queries), selects the top-m salient tokens as centroids, assigns all tokens to their nearest centroid by cosine similarity (hard clustering), and forms each compressed vector as a saliency-weighted average of tokens in the cluster (p. 5–6, Section 5.1–5.3; Fig. 2).
- Not enough information.
- This is a conceptual/positioning paper rather than an empirical study. The authors develop the IEP framing by synthesizing theories from information behaviour and HCI and then applying that framing to analyze KG exploration interfaces at the level of “interaction primitives” and their epistemic preconditions.  First, they situate the IEP relative to established concepts: Belkin’s Anomalous State of Knowledge (ASK), exploratory search, sensemaking, information foraging, onboarding/orientation, serendipitous discovery, and cognitive load theory. They argue that none fully captures the first-contact condition because many presuppose a starting point and/or a goal, whereas the IEP is defined by the absence of both (p. 2–4, Section 2; Table 1).  Second, they articulate the IEP as the co-occurrence of three interdependent barriers at first contact: scope uncertainty (not knowing what the KG contains or where to begin), ontology opacity (difficulty interpreting the conceptual schema/ontology), and query incapacity (difficulty using SPARQL) (p. 5–6, Section 3.4–3.5).  Third, they analyze KG exploration interfaces by decomposing them into common interaction primitives (e.g., search, facet selection, schema browsing, graph navigation, visual query construction, NL question answering) and identifying what user knowledge each primitive assumes. They argue these primitives are structurally misaligned with the IEP because they require epistemic conditions that are absent at first contact (p. 7, Section 4.2; Table 2). Based on this analysis, they propose “scope revelation” as a missing interaction primitive and discuss possible forms it could take (p. 8, Section 4.4).

### Privacy/Security
- The paper introduces HAT-Lab (Human-Agent Trust Laboratory), a high-fidelity platform for immersive behavioral experiments that places participants into realistic, goal-oriented tasks with agent assistance, while programmatically injecting AMD attacks (p. 2–5, Section III). The platform is grounded in a “Trust Boundary Framework” that decomposes user trust into three boundaries—Perception, Memory, and Action—and designs attacks that violate each boundary (p. 4, Section III.B). HAT-Lab includes nine scenarios spanning everyday and professional domains; Table I maps each scenario to a boundary, attack type, assigned task, and consequence (p. 5, Table I). The platform architecture includes an experimental frontend with realistic resources (e.g., simulated webmail, PDF resumes, code repository), an attack configuration layer, a LangChain-based orchestration backend creating isolated agent instances per participant, and a logging module capturing user inputs, UI events, and LLM traces (p. 4–5, Section III.C; Fig. 2).  A large-scale user study (N=303) was run on Prolific with a mixed design: between-subjects assignment to one of three guardrails (static disclaimer, persistent reminder, interactive alert) and within-subjects exposure to three scenarios via a block design to manage fatigue (p. 6, Section IV.A; Fig. 3–4). Primary susceptibility metrics include risk perception rate (noticing something unusual/questionable) and accurate identification rate (correctly describing the underlying attack mechanism) (p. 7–8, Section IV.D). The paper reports non-parametric statistical tests (Mann–Whitney U, Fisher’s exact) for analysis (p. 8, Section IV.D). The platform was validated for attack stimulus reproducibility (ASR), cross-model generalizability (COMET), and realism/usability via user feedback (p. 6, Section III.D; p. 16–17, Appendix D–F; Table IX; Fig. 11).

### Robustness
- The paper introduces DEEPSYNTH, a benchmark of 120 expert-authored tasks designed to evaluate web-based information synthesis with structured, verifiable outputs. Tasks require agents to browse and search the web, read multiple documents/tables, and perform analysis to produce concise JSON/dictionary outputs for automatic checking (p. 3, Section 2; p. 6, Section 3 Metrics). The benchmark is built via a multi-stage human pipeline: (a) data source identification, (b) hypothesis generation, (c) hypothesis validation via manual analysis, and (d) task formulation with intermediate steps, supporting evidence, and answers (p. 4, Section 2.2; Fig. 2). Sources are curated to be official/trustworthy and filtered for usefulness and verifiability (p. 4, Section 2.2). Tasks include a gold standard and a manually annotated reasoning chain (p. 2, Introduction; p. 5, Section 2.2 Task Formulation). A second independent annotation stage retains only tasks where both annotators’ answers match (p. 5, Section 2.2 Data Validation). Evaluation uses strict exact match (EM) on JSON keys/values and partial matching via precision/recall/F1 over key-value pairs, plus an LLM-as-a-judge “soft” metric that tolerates small semantic/numeric differences (p. 6, Section 3 Metrics; Fig. 10). The authors benchmark multiple LLMs and agentic frameworks with tool use (web search/browsing, code interpreter, document processing) and conduct analyses including tool ablations and providing ground-truth intermediate steps to test the role of planning (p. 6-8, Sections 4-5; Table 3).
- The paper models an LLM as a stochastic policy \(\pi_\theta(\cdot|x)\) over responses given prompts \(x\sim D\), with a binary verifier reward \(r(x,y)\in\{0,1\}\) (p. 3, Section 2). Per-prompt success is \(p_\theta(x)=\mathbb{E}_{y\sim\pi_\theta}[r(x,y)]\) (Eq. (1), p. 3), and pass@k is \(J_k(\theta)=\mathbb{E}_{x\sim D}[1-(1-p_\theta(x))^k]\) (p. 3, Section 2). Using the chain rule, they derive the pass@k gradient \(\nabla J_k(\theta)=\mathbb{E}_{x\sim D}[w_k(p_\theta(x))\nabla p_\theta(x)]\) with weights \(w_k(p)=k(1-p)^{k-1}\), emphasizing low-success prompts (Eq. (2)–(3), p. 4). To capture cross-prompt interactions under shared parameters, they define a prompt-gradient similarity kernel \(\kappa_\theta(x,x')=\langle \nabla p_\theta(x),\nabla p_\theta(x')\rangle\) (Eq. (5), p. 5) and define positive/negative prompt interference by the sign of this kernel (Definition 3.1, p. 5). They then analyze gradient conflict via the inner product \(\langle \nabla J_k(\theta),\nabla J_1(\theta)\rangle\), introducing an agreement score \(a_\theta(x)=\langle \nabla J_1(x;\theta),\nabla J_1(\theta)\rangle\) (Eq. (8), p. 7) and proving \(\langle \nabla J_k,\nabla J_1\rangle=\mathbb{E}[w_{k,\theta}(x)a_\theta(x)]\) plus an equivalent covariance decomposition (Proposition 4.1, Eq. (9)–(10), p. 7). Under a smoothness assumption on the policy (Assumption 4.3, p. 8), they provide sufficient conditions for conflict (Corollary 4.4, p. 10), study how increasing k can induce a threshold beyond which conflict occurs (Proposition 4.5, p. 10), and prove that under an explicit step-size condition, one-step pass@k gradient ascent can simultaneously increase pass@k while decreasing pass@1 (Proposition 4.6, p. 11). Empirically, they estimate agreement scores and pass@k weights on MATH using gradients w.r.t. the final hidden layer and Monte Carlo estimates (p. 11–12, Section 5).
- SELAUR is a reinforcement learning framework that injects uncertainty estimates into reward shaping for LLM agents. It has three modules: (1) token-level uncertainty estimation, (2) aggregation to step- and trajectory-level signals, and (3) failure-aware reward shaping (Fig. 1; p. 4 Section 3). For uncertainty estimation, SELAUR computes three token-level metrics from the model’s token probability distribution: normalized entropy over the vocabulary, least-confidence defined as 1 minus the probability of the chosen token, and a margin-based metric based on the gap between the top-2 token probabilities passed through a sigmoid with scaling (p. 5 Section 3.1). These are combined into a weighted sum to form a unified token-level uncertainty (p. 5 Section 3.1). Token uncertainties are averaged within each step to obtain step uncertainty, and step uncertainties are aggregated into a trajectory uncertainty using an exponentially discounted weighting that emphasizes later steps (p. 5 Section 3.2). For reward shaping, SELAUR modifies rewards primarily in failure cases: when a trajectory fails, each step reward is replaced/augmented with a step-dependent weight times normalized step uncertainty (with wt set to 0.95 in their settings), and the trajectory reward is set to the trajectory uncertainty U(τ); otherwise, standard rewards are used for successful trajectories (Eq. 1–2; p. 6 Section 3.3). Experiments compare against PPO, RLOO, GRPO, and GiGPO, reporting success rate (and WebShop task score) with specified training hyperparameters and interaction step limits (p. 6 Section 4.1).
- The paper formalizes MA-IL in n-player discounted Markov games with product policies and uses occupancy measures (state-only and state-action) to connect imitation objectives to induced behavior (p. 3, Section 3.1). It defines exploitability via the Nash gap, i.e., the maximum over players of the value improvement from a best response against the learned joint policy (p. 4, Definition 4). The analysis proceeds in three stages. (1) Exact measure matching: the authors prove that if two policies have identical state-action occupancy measures, they coincide on the visited region (Theorem 1, p. 5; proof in Appendix B.1). Under full-state support, exact state-action matching implies the learned policy equals the expert Nash equilibrium, yielding zero Nash gap (Corollary 1, p. 5). They then construct counterexamples showing that state-only matching can still produce Nash gap linear in the effective horizon (Lemma 1, p. 5–6, with Fig. 1), and that even exact state-action matching can fail when the expert does not visit all states (Theorem 2, p. 6). (2) Intractability of tight lower bounds: they define a “tight Nash gap lower bound” m_rho(G, eps_rho) as the best achievable Nash gap among policies at a fixed occupancy-matching error, and show computing it is PPAD-hard for bimatrix games (Theorem 3, p. 7), extending to Markov games (Corollary 2, p. 7). (3) Tractable upper bounds: they introduce δ-continuity of the best-response correspondence at equilibrium (Definition 6, p. 8) and derive Nash gap upper bounds in terms of BC error and δ. Under dominant strategy equilibria (Definition 8, p. 9), they obtain NashGap(π) ≤ 2n ε_BC/(1−γ)^2 (Lemma 3, p. 9; proof Appendix C.1). More generally, for δ-continuous games they show NashGap(π) ≤ (2n ε_BC + δ(ε_BC))/(1−γ)^2 (Lemma 4, p. 9; proof Appendix C.2).
- The proposed framework, Generative Pseudo-Labeling (GPL), generates content-aware pseudo-labels for unexposed user–item pairs and trains the pre-ranker with a joint objective over actual and pseudo labels (p. 2–5, Sections 3.2–3.4). First, items are tokenized into discrete semantic identifiers (SIDs) using only content: a frozen multimodal encoder produces item embeddings, and an RQ-VAE quantizes embeddings into hierarchical code indices (SIDs), trained with reconstruction/commitment losses to avoid interaction-induced bias (p. 3, Section 3.3.1; p. 10, Appendix A.2). Second, a pre-trained open-source LLM is adapted with LoRA (rank=8) and trained with next-token prediction to predict the next likely SID from a user’s SID history; popularity bias is reduced via frequency-based downsampling of the top 10% most frequent items (p. 3–4, Section 3.3.2). At inference, hierarchical beam search decodes B candidate SIDs, which are resolved to real items via a lookup table; out-of-vocabulary SIDs fall back to nearest-neighbor retrieval in the frozen multimodal space (p. 4, Section 3.3.2). These generated items serve as user-specific “interest anchors.” For each unexposed candidate, GPL computes a pseudo-label as a sigmoid of the maximum cosine similarity between the candidate and any anchor in the frozen multimodal embedding space (max-pooling), with a temperature parameter τ (p. 4, Section 3.3.3). Third, GPL calibrates pseudo-label reliability using uncertainty-aware weights derived from (i) semantic dispersion among anchors, (ii) historical consistency with the user’s past items, and (iii) LLM intrinsic confidence (average log-probability along the decoding path), combined into a normalized weight (p. 4–5, Section 3.4.1). Finally, the pre-ranker is trained with BCE on exposed data and confidence-weighted BCE on unexposed pseudo-labeled data, combined as L = L_al + λ L_pl, while other components remain frozen (p. 5, Section 3.4.2). All LLM inference and pseudo-label generation are performed offline and cached, aiming for zero online latency overhead (p. 2, Introduction; p. 5, Section 3.5).

---

## 3. Trends & Insights

### Hot topics
- Hallucination/Factuality (2 papers)
- Other (14 papers)
- Privacy/Security (1 papers)
- Robustness (5 papers)

### Technical trends
- Growing focus on agent reliability: process audits, communication pitfalls, and runtime controls.
- Methodology & engineering around PEFT and model merging remain active (hyperparameter confounds, layer-wise heuristics).

### Cross-area connections
- Security/privacy is increasingly treated as a systems problem for tool-using agents (mediation layers, auditability, governance signals).
- Cost/latency concerns (reasoning efficiency, CoT compression) intersect with evaluation methodology and deployment constraints.

---

## 4. Top 5 Recommended Papers

Ranked by overall significance and insightfulness (based on available evidence):

### 1. Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training
**Category**: Robustness
**Importance**: ⭐⭐⭐⭐
**Authors**: Anas Barakat, Souradip Chakraborty, Khushbu Pahwa, Amrit Singh Bedi
**Link**: http://arxiv.org/abs/2602.21189v1

**Core problem**:
Pass@k is a standard metric for verifiable LLM tasks (e.g., math/code) where a model can sample k independent solutions and a verifier checks correctness; success is declared if any sample passes (p. 3, Section 2). This metric has motivated post-training methods that directly optimize pass@k via policy gradients, aiming to match the inference-time procedure (p. 1, Section 1; p. 4, Section 2). However, multiple prior works report a recurring and practically problematic trade-off: optimizing pass@k can increase pass@k while decreasing pass@1 (single-shot success) (p. 1, Section 1). The paper argues this matters because pass@1 often remains an operational constraint due to latency/cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback (p. 2, “Why this degradation matters.”). The core problem is therefore to explain when and why inference-aware optimization for multi-sample success (pass@k) can harm single-sample performance (pass@1), despite the fact that per-prompt pass@k gradients are positive scalings of per-prompt pass@1 gradients (p. 7, Section 4.1; p. 4, Section 2). The authors frame the open question as understanding the mechanism behind this degradation and providing conditions under which it occurs, so that future inference-aware fine-tuning can avoid sacrificing pass@1 while improving multi-attempt performance (p. 2, “Open question.”; p. 15, Section 7).

**Method novelty**:
The paper models an LLM as a stochastic policy \(\pi_\theta(\cdot|x)\) over responses given prompts \(x\sim D\), with a binary verifier reward \(r(x,y)\in\{0,1\}\) (p. 3, Section 2). Per-prompt success is \(p_\theta(x)=\mathbb{E}_{y\sim\pi_\theta}[r(x,y)]\) (Eq. (1), p. 3), and pass@k is \(J_k(\theta)=\mathbb{E}_{x\sim D}[1-(1-p_\theta(x))^k]\) (p. 3, Section 2). Using the chain rule, they derive the pass@k gradient \(\nabla J_k(\theta)=\mathbb{E}_{x\sim D}[w_k(p_\theta(x))\nabla p_\theta(x)]\) with weights \(w_k(p)=k(1-p)^{k-1}\), emphasizing low-success prompts (Eq. (2)–(3), p. 4). To capture cross-prompt interactions under shared parameters, they define a prompt-gradient similarity kernel \(\kappa_\theta(x,x')=\langle \nabla p_\theta(x),\nabla p_\theta(x')\rangle\) (Eq. (5), p. 5) and define positive/negative prompt interference by the sign of this kernel (Definition 3.1, p. 5). They then analyze gradient conflict via the inner product \(\langle \nabla J_k(\theta),\nabla J_1(\theta)\rangle\), introducing an agreement score \(a_\theta(x)=\langle \nabla J_1(x;\theta),\nabla J_1(\theta)\rangle\) (Eq. (8), p. 7) and proving \(\langle \nabla J_k,\nabla J_1\rangle=\mathbb{E}[w_{k,\theta}(x)a_\theta(x)]\) plus an equivalent covariance decomposition (Proposition 4.1, Eq. (9)–(10), p. 7). Under a smoothness assumption on the policy (Assumption 4.3, p. 8), they provide sufficient conditions for conflict (Corollary 4.4, p. 10), study how increasing k can induce a threshold beyond which conflict occurs (Proposition 4.5, p. 10), and prove that under an explicit step-size condition, one-step pass@k gradient ascent can simultaneously increase pass@k while decreasing pass@1 (Proposition 4.6, p. 11). Empirically, they estimate agreement scores and pass@k weights on MATH using gradients w.r.t. the final hidden layer and Monte Carlo estimates (p. 11–12, Section 5).

**Key contributions**:
- Defines “prompt interference” via a pass@1 gradient-similarity kernel and formalizes positive vs negative interference (p. 5, Definition 3.1; Eq. (5)).
- Derives a characterization of pass@k vs pass@1 gradient conflict as an inner product depending on pass@k weights and a prompt agreement score, including a covariance decomposition (p. 7, Proposition 4.1).
- Provides sufficient conditions (and a k-threshold result) under which increasing k encourages gradient conflict when negatively interfering prompts are upweighted (p. 10, Corollary 4.4; Proposition 4.5).
- Proves that under gradient conflict and a step-size condition, a pass@k gradient step can increase pass@k while decreasing pass@1 (p. 11, Proposition 4.6).
- Empirically validates the mechanism on MATH with two LLMs, showing hard prompts have negative agreement and receive much larger pass@k weights, yielding negative estimated inner products (p. 11–13, Section 5; Fig. 6).

**Results**:
Theory: The paper proves that the population gradient inner product \(\langle \nabla J_k(\theta),\nabla J_1(\theta)\rangle\) equals \(\mathbb{E}[w_{k,\theta}(x)a_\theta(x)]\) and can be negative due to covariance between pass@k weights and agreement scores (p. 7, Proposition 4.1). It provides sufficient conditions where negatively interfering prompts dominate, making the inner product negative (p. 10, Corollary 4.4), and shows a threshold \(k^\star\) beyond which conflict occurs under a separation assumption on success probabilities (p. 10, Proposition 4.5). It also proves that with a small enough step size, one pass@k gradient ascent step can strictly decrease pass@1 while increasing pass@k (p. 11, Proposition 4.6).

Toy example: For a two-prompt overlap case, they report that a pass@10 step (\(\eta=5\)) decreases \(J_1\) from \(\approx 0.48\) to \(\approx 0.46\) while increasing \(J_{10}\) from \(\approx 0.83\) to \(0.95\) (p. 7, Section 3.3).

Experiments (MATH): Using DeepSeek-R1-Distill-Llama-8B and DeepSeek-R1-Distill-Qwen-7B, they compute agreement scores and pass@k weights on filtered prompt sets and show that hard prompts tend to have negative agreement while receiving much larger pass@k weights (p. 12, Section 5; Fig. 6). They report extreme weight disparity (claimed \(\sim 10^{28}:1\)) between hard and easy prompts and negative estimated inner products indicating gradient conflict; for the shown configurations, they report inner products of \(-0.613\) (Llama-8B) and \(-181\) (Qwen-7B) (p. 12–13, Section 5; Fig. 6 caption). They also report robustness across additional threshold configurations for Llama-8B with negative inner products in the range \(-0.49\) to \(-0.65\) (p. 24, Appendix D.2).

---

### 2. VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation
**Category**: Hallucination/Factuality
**Importance**: ⭐⭐⭐⭐
**Authors**: Seongheon Park, Changdae Oh, Hyeong Kyu Choi, Xuefeng Du, Sharon Li
**Link**: http://arxiv.org/abs/2602.21054v1

**Core problem**:
Large Vision-Language Models (LVLMs) can generate fluent but incorrect responses that are not supported by the input image (hallucinations), which creates safety and reliability risks in real-world deployment (p. 1, Abstract; p. 1, Introduction). A common way to improve reliability is self-evaluation: the model estimates whether its own output is correct using internal signals, avoiding costly external judges that can themselves hallucinate (p. 1, Introduction). However, existing self-evaluation methods largely originate from language-only settings and tend to rely on language priors (e.g., common textual associations) rather than verifying whether the answer is grounded in the visual evidence (p. 2, Introduction; p. 3, Section 4.1). The paper highlights a failure mode where standard uncertainty-based scores can assign low uncertainty to hallucinated answers when the image contradicts common expectations, because the model remains confident due to linguistic regularities (p. 2, Introduction; p. 1, Fig. 1 caption; p. 3, Section 4.1). This mismatch is especially problematic in counterfactual or adversarially constructed settings where visual grounding is essential. The core problem is therefore to design a training-free self-evaluation score for LVLMs that explicitly measures how much the model’s prediction depends on visual evidence, so that confidence reflects grounding rather than fluency (p. 2, Introduction; p. 3, Section 4.1).

**Method novelty**:
VAUQ is a training-free self-evaluation framework that combines predictive uncertainty with an explicit measure of visual information utilization (p. 2, Introduction; p. 4, Section 4.2; Fig. 2). First, it computes length-normalized predictive entropy H(y|v,t) over the generated sequence given image tokens v and text prompt t (p. 4, Definition 4.1 footnote). Second, it introduces an Image-Information Score (IS) that measures how much the image reduces uncertainty by comparing entropy with and without visual tokens: IS_blank = H(y|∅,t) − H(y|v,t) (p. 4, Eq. 2). To reduce sensitivity to spurious background correlations, VAUQ uses an unsupervised core-region masking strategy based on the model’s visual attention (p. 4, Section 4.2). It aggregates attention from generated tokens to image tokens across heads and a contiguous layer range (ls to le): Attn(vi) = Σ_{l=ls..le} Σ_h Σ_j A^{(l,h)}(y_j, v_i) (p. 5, Eq. 3). It then selects the top K% most-attended image patches v_top (p. 5, Eq. 4) and masks them to form v_masked, computing a core-masked information score IScore = H(y|v_masked,t) − H(y|v,t) (p. 5, Eq. 5). Finally, VAUQ defines a scoring function s_VAUQ(x,y) = H(y|v,t) − α·IScore, where α weights the penalty for weak reliance on core visual evidence (p. 5, Eq. 6). Implementation uses greedy decoding (max length 128) and, for efficiency, masks attention weights (attention knockout) rather than raw pixels when computing IScore (p. 12, Appendix A).

**Key contributions**:
- Proposes VAUQ, a training-free LVLM self-evaluation framework that explicitly accounts for reliance on visual evidence via uncertainty reduction (p. 2, Introduction; p. 4–5, Section 4.2).
- Introduces Image-Information Score (IS) and an unsupervised attention-based core-region masking strategy to better measure semantically meaningful visual utilization (p. 4–5, Section 4.2; Fig. 2).

**Results**:
VAUQ is evaluated on ViLP, MMVet, VisualCoT (free-form VQA) and CVBench (multiple-choice) across LLaVA-1.5 (7B/13B), Qwen2.5-VL-7B, and InternVL3.5-8B, using AUROC with correctness labels (GPT-5 judge for free-form; exact match for CVBench) (p. 5, Section 5.1; p. 12, Appendix A). Main results: for LLaVA-1.5-7B, VAUQ achieves AUROC 77.0 (ViLP), 81.5 (MMVet), 77.8 (VisualCoT), 73.2 (CVBench) (p. 6, Table 1). For Qwen2.5-VL-7B, VAUQ achieves 64.1/78.3/68.0/69.8 on ViLP/MMVet/VisualCoT/CVBench (p. 6, Table 2). For InternVL3.5-8B, VAUQ achieves 65.2/75.8/77.2/74.7 (p. 6, Table 2). The paper reports improvements on ViLP with LLaVA-1.5-7B: +13.4 AUROC over Semantic Entropy and +21.4 over VL-Uncertainty; and +12.6 over VL-Uncertainty on VisualCoT (p. 6, Section 5.2). Ablation on masking (VisualCoT): IS_GT (oracle evidence masking) yields higher AUROC than IS_blank, random masking degrades, and the proposed IScore is close to oracle (p. 6, Table 3). Component analysis on ViLP shows entropy strong on factual but weak on counterfactual, while IScore is the opposite; combining improves counterfactual AUROC (p. 7, Table 4). Efficiency: compared to VL-Uncertainty, VAUQ reduces per-sample inference time by 94.6% while improving AUROC by +21.4 on ViLP with LLaVA-1.5-7B (p. 8, Section 5.3; p. 7, Table 5).

---

### 3. Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking
**Category**: Other
**Importance**: ⭐⭐⭐⭐
**Authors**: Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin
**Link**: http://arxiv.org/abs/2602.21196v1

**Core problem**:
Training Transformer models on very long sequences is constrained by GPU memory, especially the activation memory required during training. Context parallelism (also called sequence parallelism) addresses this by sharding the sequence across multiple devices, enabling longer contexts than a single accelerator can handle. However, dominant context-parallel approaches such as Ring Attention and DeepSpeed-Ulysses primarily focus on scaling computation/communication over the context dimension, while the per-device activation memory still grows linearly with sequence length and can become the bottleneck at multi-million-token contexts (p. 1, Section 1). The paper argues that, beyond roughly multi-million token lengths, even with distributed sharding, the intermediate tensors in self-attention—particularly the full-head QKV tensors and the communication buffers used by all-to-all—create an “activation memory barrier” that prevents further scaling (p. 1, Section 1; p. 5, Section 3.2). Existing methods that push context further (e.g., FPDT or activation offloading) can reduce GPU memory pressure but often incur throughput penalties due to CPU involvement and data transfers (p. 1, Abstract; p. 3, Section 2.1; p. 7, Section 5.2). The core problem is therefore to extend maximum trainable context length under context parallelism by reducing attention activation memory—especially the intermediate tensors and communication buffers—while maintaining training throughput comparable to high-performance baselines like Ulysses (p. 1, Abstract; p. 2, Section 1).

**Method novelty**:
UPipe is introduced as an “untied” variant of DeepSpeed-Ulysses that reduces attention activation memory by chunking attention execution along the attention-head dimension (p. 1, Abstract; p. 5, Section 3.3). The method builds on DS-Ulysses’ approach of sharding the sequence across C devices and using all-to-all to reshard QKV from sequence-sharded to head-sharded layouts so each device can attend over the full sequence for a subset of heads (p. 4, Section 3.1). UPipe’s key change is to avoid materializing full-head QKV and full-size all-to-all buffers at once: it processes only U heads per stage (U < H), repeating for H/U stages, and reuses the same HBM buffers across stages so peak intermediate memory scales with U rather than H (p. 5, Sections 3.2–3.3; Fig. 3). The paper provides a memory analysis showing DS-Ulysses intermediate tensors are proportional to H, while UPipe replaces H with U, yielding a tunable runtime–memory tradeoff; the smallest valid U is U=C (U divisible by C) (p. 5–6, Sections 3.3–3.4). To remain compatible with Grouped Query Attention (GQA), UPipe introduces a scheduling strategy that communicates as many unique KV heads as possible early and then reuses them across subsequent stages by selecting queries “out of order,” reducing redundant KV communication (p. 6–7, Section 4.1; Fig. 4). Implementation-wise, experiments integrate UPipe into TorchTitan, use FlashAttention-3 kernels, and combine with other memory-saving components (tiled FFN/RMSNorm, fused linear+cross-entropy loss, and activation checkpointing with CPU offloading) to enable multi-million-token training (p. 6, Section 4; p. 7, Section 5.1).

**Key contributions**:
- UPipe: a headwise-chunked, multi-stage execution of Ulysses-style context parallel attention that reuses buffers to reduce peak attention activation memory (p. 2, Contributions; p. 5, Section 3.3).
- A GQA-compatible scheduling method that reorders query processing to reuse communicated KV tensors and reduce redundant communication (p. 2, Contributions; p. 6–7, Section 4.1; Fig. 4).

**Results**:
Single-node (8×H100) Llama3-8B: UPipe supports up to 5M tokens, while Ulysses and Ring OOM at 4M and FPDT fails beyond 4M; UPipe throughput at 5M is 98.25 tokens/s/GPU (Table 3, p. 8). The paper states this is a 25% improvement in maximum context length over FPDT’s 4M (p. 8, Section 5.3.1; Table 3).
Memory: At 3M tokens for Llama3-8B, a breakdown figure shows UPipe fits while several baselines OOM; UPipe peak memory is shown as 37.5 GiB total with 15.0 GiB model and 15.4 GiB attention activations (Fig. 2, p. 2; figure labels). Appendix Table 4 reports UPipe peak memory for Llama3-8B at 5M as 72.30 GiB (p. 13, Appendix A.1).
Multi-node (16×H100) Llama3-8B: UPipe supports up to 8M tokens vs USP-Hybrid up to 6M, a 33% increase, with comparable throughput (Fig. 5, p. 8; p. 8, Section 5.3.2).
Qwen3-32B (16×H100): UPipe supports 4M tokens while Ulysses supports 2M; at 4M, UPipe throughput is 29.97 tokens/s/GPU vs FPDT 27.66 (Table 3, p. 8). The paper also claims up to 87.5% reduction in attention intermediate tensor memory for Qwen3-32B at C=8 (p. 6, Section 3.4).
Ablation: varying head-chunk size U shows a memory–throughput tradeoff on Llama3-8B at 512K with C=4 (Fig. 6, p. 9), but exact numeric values are not readable from the provided parsed text.

---

### 4. Tool Building as a Path to "Superintelligence"
**Category**: Other
**Importance**: ⭐⭐⭐⭐
**Authors**: David Koplow, Tomer Galanti, Tomaso Poggio
**Link**: http://arxiv.org/abs/2602.21061v1

**Core problem**:
The paper investigates a key empirical bottleneck in the “Diligent Learner” framework: whether large language models (LLMs) can sustain a non-vanishing per-step success probability (denoted γ) as multi-step reasoning depth increases. In the Diligent Learner view, reasoning is modeled as validator-guided search over semantic steps; test-time search can scale to solve bounded-depth problems only if the model’s proposal distribution keeps enough probability mass on “good” next steps at each depth (p. 5, Section 3.1). While the theory provides guarantees when γ stays bounded away from zero, it remains unclear whether γ collapses on out-of-distribution (OOD) logical inference tasks that require integrating accumulated state with new evidence at every step (p. 1–2, Introduction).

The authors argue existing reasoning benchmarks are inadequate for measuring γ because they often score only final answers, allow multiple valid intermediate trajectories, or permit shortcut strategies such as pattern matching or memorization of benchmark regularities (p. 2, Introduction; p. 4–5). To address this, they design a benchmark where each step has a unique correct continuation and where ignoring either the revealed history (prefix) or the fresh step-specific evidence is intended to be information-theoretically ineffective (p. 2; p. 4–6). The core problem matters because if γ degrades sharply with depth on such OOD tasks, then simply increasing test-time compute/search would not yield robust long-horizon reasoning; conversely, if γ can be stabilized (e.g., via tools), it supports the feasibility of the Diligent Learner pathway to highly capable agents (p. 5; p. 14–16).

**Method novelty**:
The paper introduces a stepwise Boolean function reconstruction task over GF(2), where the target function is represented in Algebraic Normal Form (ANF) as an XOR of monomials. Inputs are split into address bits a and payload bits v, and each ANF term has the form tj(a,v)=aj·Mj(v), where Mj(v) is a degree-(d−1) monomial over a subset Sj of payload variables (p. 6, Section 4.1, Eq. 3). An instance is an ordered sequence of supports (S1,…,Sn), making the next correct term unique at each step (p. 6, Section 4.1).

At step g, the model receives (i) the prefix Pg=(t1,…,tg) and (ii) a fresh labeled dataset Sg sampled from a step-g oracle; it must output the next monomial tg+1. Step-success is defined as γg = Pr[ t̂ = tg+1 ] under the model’s stochastic policy conditioned on (Pg,Sg) (p. 6, Section 4.1, Eq. 4). To prevent shortcuts, the authors design a “statistical obfuscation” sampling oracle: labels decompose into a prefix-dependent mask XOR a next-term signal, so data-only solvers see near-random labels unless they condition on the prefix (p. 8, Section 4.2, Eq. 6; Lemma 4.3). They also sample supports i.i.d. so the prefix provides no information about the next support, defeating history-only prediction (p. 7, Section 4.2, Lemma 4.1). Payloads are drawn from a fixed Hamming-weight sphere and a weight w* is chosen to make monomial firing probability near 1/2 to reduce label bias (p. 7–8, Section 4.2, Lemma 4.2).

For evaluation, they convert the task into a single-turn prompt containing metadata, the prefix terms, and a table of 32 labeled observations; the prompt specifies the active address variable and constrains the search to payload indices (p. 9, Section 5.1). Validation is efficient because there is exactly one valid continuation; correctness is checked by parsing the predicted indices and comparing sets (p. 10, Section 5.2). They also provide a polynomial-time decoder for diligent solvers via residualization and intersection over positive examples (Appendix B, Eq. 7–8; Theorem B.1).

**Key contributions**:
- A stepwise GF(2)/ANF circuit reconstruction benchmark that makes the correct next step unique and directly measures per-step success γg (p. 2; p. 6, Section 4.1).
- A statistical obfuscation sampling oracle with theoretical guarantees intended to defeat history-only and data-only shortcut strategies (p. 7–9, Section 4.2; Lemmas 4.1–4.3).
- Empirical evaluation showing depth-induced collapse of γg for smaller LLMs and substantially higher, more stable γg for frontier models when tool calls are allowed (p. 11–15, Section 6; Fig. 5–7).
- A polynomial-time recovery procedure for diligent solvers and an efficient validator enabling scalable stepwise evaluation (Appendix B, Theorem B.1; p. 10, Section 5.2).

**Results**:
Estimator simulations: Figure 3 reports γg versus depth for four estimator classes, showing the diligent estimator (history+data) maintains high γg across depths while data-only and partial estimators collapse toward near-zero and history-only stays at chance (p. 10, Fig. 3 caption). Figure 4 shows a heatmap over depth g and payload dimension p where only the diligent estimator remains consistently high as g and p increase (p. 11, Fig. 4 caption).

Small LLMs: Using Qwen3-2507 models evaluated on 3000 generated instances across depths g ∈ {1,3,7,15,31} with p=12 and d=4, the paper reports that all tested small models show systematic decline in γg with depth; 4B-Instruct is not statistically distinguishable from random even at the easiest setting (p. 11–13, Section 6.2; Fig. 5 caption). The 30B “Thinking” variant performs better at shallow depths but drops sharply around intermediate depths (around g=15) and approaches a trivial baseline (p. 11–13, Section 6.2).

Frontier LLMs: For GPT 5.2 (extended Thinking), Claude Opus 4.5 (max Thinking), and Gemini 3 Pro (Jan 2026), results are reported from 60 queries per model across g ∈ {31,63,127} with p=12 and d=4, comparing tool-allowed vs instructed-no-tools prompts (p. 13, Section 6.3). The paper states frontier models remain strong where small LLMs are at random (p. 13, Section 6.3; Fig. 6 caption) and that tool-enabled frontier models maintain near-unity γg even at g=127 with minimal degradation, while no-tool performance drops substantially (p. 14–15; Fig. 7 caption). The authors note Opus may have used tools even when instructed not to, potentially inflating its no-tools score (p. 15, Fig. 7 caption; p. 16).

---

### 5. Test-Time Training with KV Binding Is Secretly Linear Attention
**Category**: Other
**Importance**: ⭐⭐⭐⭐
**Authors**: Junchen Liu, Sven Elflein, Or Litany, Zan Gojcic, Ruilong Li
**Link**: http://arxiv.org/abs/2602.21204v1

**Core problem**:
Test-time training (TTT) has evolved from a technique for adapting models under distribution shift into a sequence-modeling architectural primitive that can serve as an alternative to softmax attention, with favorable efficiency properties such as linear-time compute and constant memory during autoregressive inference (p. 1, Section 1). A prominent variant, TTT with key–value binding (TTT-KVB), is widely interpreted as an online meta-learning / fast-weights mechanism that “memorizes” a key–value mapping in an inner loop and then “retrieves” information by applying the updated fast-weight function to queries (p. 1, Section 1; p. 3, Section 3). This storage-and-retrieval interpretation has driven increasingly complex architectural choices (e.g., sophisticated optimizers, normalization, deeper inner-loop networks) intended to improve memorization fidelity (p. 1, Section 1).

The paper argues that this prevailing interpretation is inconsistent with observed behavior in trained TTT models. The authors identify several empirical “paradoxes” that contradict what one would expect if the inner loop were truly performing useful memorization and query-based retrieval: downstream performance can worsen as inner-loop fitting improves; gradient ascent can replace gradient descent without harming performance; queries and keys can be distributionally mismatched; and replacing queries with keys can have negligible effect (p. 1, Section 1; p. 3–4, Section 4). These anomalies matter because they suggest that current design intuitions for TTT may be misguided, potentially leading to unnecessary complexity and missed opportunities for simplification and acceleration. The core problem is therefore to correctly characterize what computation TTT-KVB layers implement, and to use that characterization to explain the paradoxes and enable more principled, efficient designs (p. 2, Section 1; p. 4–6, Section 5; p. 7–8, Section 6).

**Method novelty**:
The paper combines empirical probes with analytical reformulation. Empirically, it tests predictions of the memorization view by manipulating inner-loop behavior at inference time and measuring downstream task metrics. Examples include varying the number of inner-loop gradient steps (showing inner loss improves while task performance degrades; Fig. 1, p. 3–4, Section 4.1), swapping gradient descent for gradient ascent (Table 1, p. 4, Section 4.2), visualizing query/key distributions via t-SNE to assess distributional overlap (Fig. 2, p. 4, Section 4.3), and replacing queries with keys when computing outputs (Table 1, p. 4, Section 4.4).

Analytically, the authors “unroll” the inner-loop updates and prove that a broad class of TTT-KVB architectures can be rewritten as a learned linear-attention-like operator. The key technical condition is that the inner-loop function has a linear, bias-free final layer f(x)=ϕ(x;Θ)W (p. 5, Theorem 5.1). After one gradient step on a key input, the updated output on a query can be expressed in a linear attention form o = q̂ (S0 + k̂^T v̂), where q̂, k̂, v̂ are induced by the network features and the loss gradient (p. 5, Theorem 5.1). Repeating over a sequence yields an “extended” linear attention accumulation over history (p. 5, Theorem 5.2). The framework is extended to gradient descent with momentum, yielding momentum-weighted effective values (p. 5, Theorem 5.3). They then instantiate the reduction for specific TTT variants: LaCT (with SwiGLU inner-loop MLP, Frobenius inner-product loss, per-token learning rates, momentum, and gradient orthogonalization) is rewritten in linear-attention-like form (p. 6, Section 5.3; Appendix E), and ViTTT’s GLU and depthwise convolution fast-weight components are similarly expressed as linear-attention-like mechanisms (p. 6–7, Section 5.4; Appendix F–G).

Finally, they use this perspective to define an ablation trajectory that progressively removes or simplifies inner-loop components (e.g., update only last layer, remove weight normalization, reduce MLP depth, remove per-token learning rates, remove momentum, remove gradient orthogonalization) and evaluate performance/throughput, including a parallel formulation enabled when associativity holds (p. 7–8, Section 6; Table 2; Appendix H–I).

**Key contributions**:
- Empirically identifies multiple behaviors of TTT-KVB (e.g., gradient ascent works; inner-loop fitting can hurt; Q/K mismatch; Q→K replacement) that contradict a storage-and-retrieval interpretation (p. 1, Section 1; p. 3–4, Section 4; Table 1).
- Proves that a broad class of TTT-KVB inner-loop updates (including multi-layer inner loops with a linear bias-free final layer, and momentum) can be rewritten as a learned linear attention operator via unrolling (p. 5, Theorems 5.1–5.3).
- Derives concrete linear-attention forms for representative architectures (LaCT and ViTTT components), connecting practical TTT designs to the unified formulation (p. 6–7, Sections 5.3–5.4; Appendix E–G).
- Shows practical benefits of the linear-attention view: principled simplifications via an ablation trajectory and a parallel implementation that improves throughput while maintaining performance (p. 7–8, Section 6; Table 2; Appendix H).

**Results**:
The paper reports that (i) increasing inner-loop iterations improves inner-loop loss but degrades downstream performance (Fig. 1, p. 3, Section 4.1); (ii) replacing gradient descent with gradient ascent yields comparable or slightly better performance across evaluated models/tasks (Table 1, p. 4, Section 4.2); (iii) replacing queries with keys has little effect on performance (Table 1, p. 4, Section 4.4); and (iv) t-SNE visualizations show a pronounced mismatch between Q and K distributions in a pretrained LaCT model on NVS (Fig. 2, p. 4, Section 4.3).

In the simplification/ablation trajectory, updating only the last-layer parameters (Variant 1) achieves the best reported performance across the three tasks in Table 2 (p. 8, Section 6.1). Reducing to standard linear attention (Variant 6) causes only minor degradation relative to baseline on the reported metrics (p. 8, Section 6.1; Table 2). The parallel implementation (for variants where associativity holds) improves TTT-layer inference throughput substantially; Table 2 reports tokens/sec for recurrent vs parallel implementations, and the text states up to 4.0× throughput improvement on attention calculation while maintaining performance (p. 2, Section 1; p. 8, Section 6.2; Table 2). The paper also reports a 1.19× end-to-end training speedup for LaCT-LLM when using the parallel form of Variant 2 with comparable convergence (p. 8, Section 6.2; Fig. 4).

---

## 5. Summary & Outlook

### Today’s highlights
- This report summarizes 22 deep-read analyses.
- Covered categories: Hallucination/Factuality, Other, Privacy/Security, Robustness

### Key progress
- Multiple papers target agent reliability and safety governance at the system level (auditing, mediation, telemetry).
- Reasoning cost and evaluation methodology continue to be first-class research topics.

### Future directions
- Combine process-level auditing signals with runtime policies to build deployable 'agent firewall + audit' loops.
- Validate generalization under real tool environments and adversarial conditions; clearly report when evidence is missing.

---

*Report generated at: 2026-02-25 23:23:52*
