---
layout: single
title: "Daily AI Paper Report (2026-02-27)"
date: 2026-02-27
permalink: /paper-news/2026-02-27/
author_profile: true
---


> Chinese version: [[中文]](/paper-news/2026-02-27-zh/)


<div class="paper-news-stats">
  <p><strong>Run stats</strong></p>
  <ul>
    <li><strong>Candidates</strong>: 262</li>
    <li><strong>Selected</strong>: 30</li>
    <li><strong>Deepread completed</strong>: 30</li>
    <li><strong>Window (UTC)</strong>: 2026-02-26T01:00:00Z → 2026-02-27T01:00:00Z <em>(arxiv_announce, expanded=0)</em></li>
  </ul>
</div>

<details class="paper-news-details"><summary><strong>Show selected papers</strong></summary><div style="overflow-x:auto;"><table class="paper-news-table"><thead><tr><th>arXiv ID</th><th>Title / Links</th><th>Categories</th><th>Score</th><th>Why</th><th>Tags</th></tr></thead><tbody><tr><td><code>2602.22755</code></td><td><a href="https://arxiv.org/abs/2602.22755v1">AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors</a><br/><a href="https://arxiv.org/pdf/2602.22755v1.pdf">PDF</a></td><td>cs.CL</td><td>96</td><td>Benchmark of hidden misalignment behaviors + agentic auditing; strong for eval & oversight research</td><td>alignment auditing, benchmark, hidden behaviors, model evaluation, agent tools, deception</td></tr><tr><td><code>2602.23329</code></td><td><a href="https://arxiv.org/abs/2602.23329v1">LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</a><br/><a href="https://arxiv.org/pdf/2602.23329v1.pdf">PDF</a></td><td>cs.AI, cs.CL, cs.CR, cs.CY, cs.HC</td><td>96</td><td>Careful human uplift study on bio dual-use tasks; quantifies novice capability jump with LLM access.</td><td>dual-use, biosecurity, human-uplift, evaluation, risk-assessment, LLMs</td></tr><tr><td><code>2602.22724</code></td><td><a href="https://arxiv.org/abs/2602.22724v1">AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification</a><br/><a href="https://arxiv.org/pdf/2602.22724v1.pdf">PDF</a></td><td>cs.CR, cs.AI</td><td>94</td><td>Directly targets indirect prompt injection in agents with trajectory-aware detection/mitigation</td><td>agent security, prompt injection, tool outputs, inference-time defense, causal diagnostics, context sanitization</td></tr><tr><td><code>2602.22525</code></td><td><a href="https://arxiv.org/abs/2602.22525v1">Systems-Level Attack Surface of Edge Agent Deployments on IoT</a><br/><a href="https://arxiv.org/pdf/2602.22525v1.pdf">PDF</a></td><td>cs.CR</td><td>94</td><td>Empirical security analysis of edge LLM agents; defines measurable system security metrics + failures.</td><td>agent-security, edge-agents, IoT, attack-surface, systems-security, provenance, MQTT</td></tr><tr><td><code>2602.22557</code></td><td><a href="https://arxiv.org/abs/2602.22557v1">CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety</a><br/><a href="https://arxiv.org/pdf/2602.22557v1.pdf">PDF</a></td><td>cs.AI, cs.LG</td><td>92</td><td>Zero-shot safety policy adaptation via RAG + adversarial debate grounded in policy docs</td><td>LLM safety, policy compliance, RAG, multi-agent debate, governance, zero-shot</td></tr><tr><td><code>2602.22787</code></td><td><a href="https://arxiv.org/abs/2602.22787v1">Probing for Knowledge Attribution in Large Language Models</a><br/><a href="https://arxiv.org/pdf/2602.22787v1.pdf">PDF</a></td><td>cs.CL, cs.AI</td><td>92</td><td>Probe predicts whether outputs rely on prompt vs internal knowledge; useful for hallucination mitigation.</td><td>hallucinations, attribution, interpretability, faithfulness, factuality, probing</td></tr><tr><td><code>2602.22953</code></td><td><a href="https://arxiv.org/abs/2602.22953v1">General Agent Evaluation</a><br/><a href="https://arxiv.org/pdf/2602.22953v1.pdf">PDF</a></td><td>cs.AI</td><td>92</td><td>Proposes unified protocol + framework for general-agent evaluation; addresses benchmark integration bias.</td><td>agent-evaluation, benchmarks, protocols, framework, general-agents, measurement</td></tr><tr><td><code>2602.22603</code></td><td><a href="https://arxiv.org/abs/2602.22603v1">SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning</a><br/><a href="https://arxiv.org/pdf/2602.22603v1.pdf">PDF</a></td><td>cs.AI, cs.LG</td><td>92</td><td>LRM-driven KV cache compression for long-horizon agents; targets real bottleneck in agentic reasoning.</td><td>agents, long-context, KV-cache, memory, efficiency, reasoning</td></tr><tr><td><code>2602.22554</code></td><td><a href="https://arxiv.org/abs/2602.22554v1">Multilingual Safety Alignment Via Sparse Weight Editing</a><br/><a href="https://arxiv.org/pdf/2602.22554v1.pdf">PDF</a></td><td>cs.LG</td><td>90</td><td>Training-free multilingual safety alignment via sparse weight editing; addresses cross-lingual guardrail gaps</td><td>multilingual safety, weight editing, safety neurons, alignment, low-resource languages, robustness</td></tr><tr><td><code>2602.22576</code></td><td><a href="https://arxiv.org/abs/2602.22576v1">Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training</a><br/><a href="https://arxiv.org/pdf/2602.22576v1.pdf">PDF</a></td><td>cs.CL, cs.IR, cs.LG</td><td>89</td><td>Reward shaping for agentic RAG RL; extracts signal from failed trajectories to improve sample efficiency.</td><td>agentic-RAG, reinforcement-learning, reward-shaping, retrieval, training, reasoning</td></tr><tr><td><code>2602.23271</code></td><td><a href="https://arxiv.org/abs/2602.23271v1">Evaluating Stochasticity in Deep Research Agents</a><br/><a href="https://arxiv.org/pdf/2602.23271v1.pdf">PDF</a></td><td>cs.AI</td><td>89</td><td>Formalizes and measures stochasticity/variance in research agents; identifies sources via MDP framing.</td><td>agents, evaluation, stochasticity, reliability, deep-research, variance</td></tr><tr><td><code>2602.22556</code></td><td><a href="https://arxiv.org/abs/2602.22556v1">Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation</a><br/><a href="https://arxiv.org/pdf/2602.22556v1.pdf">PDF</a></td><td>cs.LG, cs.AI, cs.CL</td><td>89</td><td>Adaptive thinking RL to curb overthinking while preserving hard-query reasoning; practical reliability gain.</td><td>reasoning, RL, efficiency, overthinking, post-training, LRM</td></tr><tr><td><code>2602.22775</code></td><td><a href="https://arxiv.org/abs/2602.22775v1">TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation</a><br/><a href="https://arxiv.org/pdf/2602.22775v1.pdf">PDF</a></td><td>cs.HC, cs.AI, cs.CL</td><td>88</td><td>Multi-agent adversarial simulation to surface long-horizon relational safety failures in therapy chatbots</td><td>conversational safety, mental health, multi-turn evaluation, adversarial simulation, relational harms, red teaming</td></tr><tr><td><code>2602.22675</code></td><td><a href="https://arxiv.org/abs/2602.22675v1">Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization</a><br/><a href="https://arxiv.org/pdf/2602.22675v1.pdf">PDF</a></td><td>cs.CL</td><td>87</td><td>Agentic search framework emphasizing parallel evidence gathering to cut cost and improve generalization.</td><td>agents, search, efficiency, long-horizon, generalization, deep-research</td></tr><tr><td><code>2602.22897</code></td><td><a href="https://arxiv.org/abs/2602.22897v1">OmniGAIA: Towards Native Omni-Modal AI Agents</a><br/><a href="https://arxiv.org/pdf/2602.22897v1.pdf">PDF</a></td><td>cs.AI, cs.CL, cs.CV, cs.LG, cs.MM</td><td>87</td><td>Omni-modal agent benchmark (audio+video+image+tools) with multi-hop queries; useful for capability eval.</td><td>multimodal, agents, benchmark, tool-use, evaluation, audio, video</td></tr><tr><td><code>2602.22769</code></td><td><a href="https://arxiv.org/abs/2602.22769v1">AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications</a><br/><a href="https://arxiv.org/pdf/2602.22769v1.pdf">PDF</a></td><td>cs.AI, cs.LG</td><td>86</td><td>Agent memory benchmark focused on real agent-environment trajectories, not just dialogue</td><td>agent evaluation, long-horizon memory, benchmark, trajectories, agentic systems</td></tr><tr><td><code>2602.22719</code></td><td><a href="https://arxiv.org/abs/2602.22719v1">Interpreting and Steering State-Space Models via Activation Subspace Bottlenecks</a><br/><a href="https://arxiv.org/pdf/2602.22719v1.pdf">PDF</a></td><td>cs.LG</td><td>86</td><td>Mechanistic interpretability + test-time steering for Mamba SSMs with sizable benchmark gains.</td><td>interpretability, steering, state-space-models, Mamba, mechanistic, control</td></tr><tr><td><code>2602.22968</code></td><td><a href="https://arxiv.org/abs/2602.22968v1">Certified Circuits: Stability Guarantees for Mechanistic Circuits</a><br/><a href="https://arxiv.org/pdf/2602.22968v1.pdf">PDF</a></td><td>cs.AI, cs.CV, cs.CY</td><td>85</td><td>Provable stability guarantees for mechanistic circuit discovery; improves interpretability reliability</td><td>mechanistic interpretability, circuits, certification, robustness, theory, OOD stability</td></tr><tr><td><code>2602.23200</code></td><td><a href="https://arxiv.org/abs/2602.23200v1">InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models</a><br/><a href="https://arxiv.org/pdf/2602.23200v1.pdf">PDF</a></td><td>cs.LG, cs.CL</td><td>85</td><td>Hardware-aware KV-cache quantization reducing latency/memory for long-context decoding without accuracy loss.</td><td>LLM-efficiency, KV-cache, quantization, long-context, inference, systems</td></tr><tr><td><code>2602.23193</code></td><td><a href="https://arxiv.org/abs/2602.23193v1">ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering</a><br/><a href="https://arxiv.org/pdf/2602.23193v1.pdf">PDF</a></td><td>cs.AI</td><td>84</td><td>Event-sourcing architecture for LLM agents: separates intent from state mutation for reliability/auditing.</td><td>agents, software-engineering, state, orchestration, auditability, reliability</td></tr><tr><td><code>2602.23136</code></td><td><a href="https://arxiv.org/abs/2602.23136v1">Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs</a><br/><a href="https://arxiv.org/pdf/2602.23136v1.pdf">PDF</a></td><td>cs.CL, cs.AI, cs.LG</td><td>84</td><td>Information-theoretic account of modality collapse as mismatched decoding; actionable framing for MLLMs.</td><td>multimodal-LLMs, information-theory, decoding, modality-collapse, analysis</td></tr><tr><td><code>2602.22871</code></td><td><a href="https://arxiv.org/abs/2602.22871v1">Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching</a><br/><a href="https://arxiv.org/pdf/2602.22871v1.pdf">PDF</a></td><td>cs.CL, cs.AI</td><td>84</td><td>Step-level PRM scoring and stitching for diffusion LMs; improves test-time scaling beyond voting.</td><td>test-time-scaling, diffusion-LM, process-reward-model, reasoning, self-consistency</td></tr><tr><td><code>2602.22584</code></td><td><a href="https://arxiv.org/abs/2602.22584v1">Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA</a><br/><a href="https://arxiv.org/pdf/2602.22584v1.pdf">PDF</a></td><td>cs.CL</td><td>82</td><td>Industrial RAG reliability: jointly optimizes retrieval+generation with evidence-constrained RL</td><td>RAG, hallucination reduction, faithfulness, reinforcement learning, retrieval optimization, enterprise QA</td></tr><tr><td><code>2602.22585</code></td><td><a href="https://arxiv.org/abs/2602.22585v1">Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach</a><br/><a href="https://arxiv.org/pdf/2602.22585v1.pdf">PDF</a></td><td>cs.AI, cs.LG</td><td>82</td><td>Uses IRT/Rasch to correct rater effects in human evals; improves validity of AI comparisons and RLHF data.</td><td>evaluation, human-ratings, psychometrics, RLHF, measurement, bias</td></tr><tr><td><code>2602.22642</code></td><td><a href="https://arxiv.org/abs/2602.22642v1">Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning</a><br/><a href="https://arxiv.org/pdf/2602.22642v1.pdf">PDF</a></td><td>cs.LG</td><td>82</td><td>Difficulty-aware entropy regularization to compress CoT while preserving exploration on hard problems.</td><td>reasoning, CoT, efficiency, entropy-regularization, RL, inference-cost</td></tr><tr><td><code>2602.23262</code></td><td><a href="https://arxiv.org/abs/2602.23262v1">Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling</a><br/><a href="https://arxiv.org/pdf/2602.23262v1.pdf">PDF</a></td><td>cs.CV, cs.CR</td><td>81</td><td>DP image generation via wavelet coarse-to-fine; targets privacy-sensitive frequencies to reduce quality loss.</td><td>privacy, differential-privacy, image-generation, wavelets, memorization</td></tr><tr><td><code>2602.22758</code></td><td><a href="https://arxiv.org/abs/2602.22758v1">Decomposing Physician Disagreement in HealthBench</a><br/><a href="https://arxiv.org/pdf/2602.22758v1.pdf">PDF</a></td><td>cs.AI, stat.AP</td><td>81</td><td>Finds most HealthBench label variance is irreducible case-level residual; important for eval design.</td><td>evaluation, medical-AI, rater-disagreement, uncertainty, benchmarks, reliability</td></tr><tr><td><code>2602.23258</code></td><td><a href="https://arxiv.org/abs/2602.23258v1">AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning</a><br/><a href="https://arxiv.org/pdf/2602.23258v1.pdf">PDF</a></td><td>cs.AI, cs.CL</td><td>80</td><td>Test-time rectify-or-reject pruning to prevent error cascades in multi-agent systems</td><td>multi-agent systems, test-time control, error correction, RAG, robustness, information flow</td></tr><tr><td><code>2602.23079</code></td><td><a href="https://arxiv.org/abs/2602.23079v1">Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent</a><br/><a href="https://arxiv.org/pdf/2602.23079v1.pdf">PDF</a></td><td>cs.CL, cs.CR, cs.LG</td><td>80</td><td>Stylometry+LLM agent to assess/mitigate deanonymization risk; relevant to privacy leakage from text.</td><td>privacy, deanonymization, stylometry, LLM-agents, security, text</td></tr><tr><td><code>2602.22546</code></td><td><a href="https://arxiv.org/abs/2602.22546v1">Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention</a><br/><a href="https://arxiv.org/pdf/2602.22546v1.pdf">PDF</a></td><td>cs.AI</td><td>79</td><td>Learned policy to query human experts as a tool; large gains in Minecraft on hard tasks (human-in-loop).</td><td>human-in-the-loop, agents, tool-use, collaboration, planning, Minecraft</td></tr></tbody></table></div></details>

# AI Paper Insight Brief
## 2026-02-27

### 0) Executive takeaways (read this first)
- **Agent safety is shifting “down the stack”**: multiple papers show that deployment architecture (edge IoT swarms, tool-return boundaries, KV/memory management) can dominate risk/robustness outcomes, often bypassing prompt/model-level defenses.
- **Inference-time, training-free interventions are maturing** across safety and efficiency: causal counterfactual defenses for indirect prompt injection (ASR reported 0%), policy-grounded debate for zero-shot policy swaps, and sparse weight edits for multilingual safety transfer.
- **GRPO is becoming a default backbone** for both capability and safety/faithfulness tuning (adaptive thinking, agentic RAG reward shaping, industrial RAG faithfulness, human-collaboration modules), with new work focusing on stabilizing gradients and rewards under length/path heterogeneity.
- **Long-horizon agents are hitting systems bottlenecks** (KV cache growth, memory retrieval failures, stochasticity across runs). New benchmarks and mechanisms (AMA-Bench, stochasticity variance metrics, SideQuest) make these failure modes measurable and optimizable.
- **Evaluation methodology is under active repair**: rater-effect modeling (MFRM/IRT) and physician-disagreement decomposition show that raw human labels can reorder system rankings and that much disagreement is case-specific—implying “better judges” may require better task design, not just better models.
- **Biosecurity uplift evidence is now human-subject, multi-model, long-horizon**: novices with LLM access were reported 4.16× more accurate than internet-only novices, and most reported little difficulty overcoming safeguards—raising the priority of realistic uplift evaluations.

### 2) Key themes (clusters)

### Theme: Inference-time safety layers for agents (policy, prompt injection, edge systems)

- **Why it matters**: As agents act through tools and physical systems, the critical failures often occur at boundaries (tool returns, messaging buses, fallback paths) where classic prompt defenses don’t apply or aren’t observable.
- **Representative papers**:
  - [AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification](https://arxiv.org/abs/2602.22724v1)
  - [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557v1)
  - [Systems-Level Attack Surface of Edge Agent Deployments on IoT](https://arxiv.org/abs/2602.22525v1)
- **Common approach**:
  - Move defenses to **decision boundaries** (tool-return checkpoints; policy-grounded adjudication; MQTT control plane).
  - Use **structured protocols** (retrieval-grounded debate verdicts; causal counterfactual regimes; provenance metadata envelopes).
  - Emphasize **measurable operational metrics** (ASR/UA/FPR; latency; actuation-to-audit delay; egress/sovereignty; failover windows).
- **Open questions / failure modes**:
  - Overhead/latency: counterfactual re-executions and multi-agent debate increase inference cost.
  - Backbone brittleness: formatting adherence issues can break parsing (CourtGuard); edge heterogeneity complicates enforcement (IoT).
  - Trust boundary gaps: MQTT brokers accepting spoof/replay/direct publishes; silent fallback crossing sovereignty boundaries.

### Theme: RL (often GRPO) for agentic RAG, faithfulness, and collaboration

- **Why it matters**: Agentic systems need dense learning signals beyond final-answer correctness; industrial deployments also need faithfulness constraints (e.g., URL hallucination) that are operationally testable.
- **Representative papers**:
  - [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576v1)
  - [Towards Faithful Industrial RAG: A Reinforced Co-adaptation Framework for Advertising QA](https://arxiv.org/abs/2602.22584v1)
  - [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546v1)
- **Common approach**:
  - Replace sparse outcome rewards with **process/path rewards** (dual-track step coverage; soft outcome scoring).
  - Use **GRPO-style RL** plus structured formats (planner-first trajectories; tagged interaction protocols).
  - Add **domain-specific faithfulness constraints** (evidence faithfulness; URL validity checks with penalties).
- **Open questions / failure modes**:
  - Dependence on **LLM evaluators/judges** for scoring (reward hacking / evaluator sensitivity).
  - Offline artifacts: reference planners and indicator-like resources add pipeline complexity.
  - Generalization: training often anchored in specific domains (Minecraft, advertising QA, QA benchmarks).

### Theme: Reasoning efficiency without accuracy collapse (adaptive thinking, entropy/length control)

- **Why it matters**: Long CoT is expensive; naive length penalties can collapse exploration or destabilize RL due to extreme length heterogeneity.
- **Representative papers**:
  - [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556v1)
  - [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642v1)
  - [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871v1)
- **Common approach**:
  - **Instance-adaptive control** (think/no-think token; hard/easy entropy scaling; per-question shortest-correct length baselines).
  - Stabilize RL with **advantage shaping + gradient reweighting** under length heterogeneity.
  - Shift test-time scaling from “one long trace” to **step-level reuse** (PRM-scored stitching + AR recomputation).
- **Open questions / failure modes**:
  - Scaling beyond small/medium models not established in some work (adaptive thinking evaluated on 1.5B/7B).
  - Reliance on PRMs and diversity of sampled traces; shared mistakes limit stitching recovery.
  - Difficulty estimation proxies (historical accuracy EMA) may be brittle across domains.

### Theme: Long-horizon agent infrastructure: memory, KV cache, stochasticity, and evaluation

- **Why it matters**: As agents run longer, failures become systems failures: memory compression loses causal state, KV cache becomes a serving bottleneck, and stochasticity undermines reliability even at temperature 0 in API settings.
- **Representative papers**:
  - [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603v1)
  - [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769v1)
  - [Evaluating Stochasticity in Deep Research Agents](https://arxiv.org/abs/2602.23271v1)
  - [InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models](https://arxiv.org/abs/2602.23200v1)
- **Common approach**:
  - Make hidden bottlenecks **measurable** (peak token utilization, KV reads, total variance over findings/citations, needle protocols).
  - Use **model-driven or hardware-aligned** mechanisms (aux-thread semantic eviction; inner-dimension KV grouping).
  - Add **structured mitigations** (structured outputs; query-intersection ensembling; tool-augmented retrieval over causality graphs).
- **Open questions / failure modes**:
  - OOD degradation (SideQuest up to 5% accuracy drop on BrowseComp).
  - Memory construction/retrieval losses dominate end-to-end performance (AMA-Bench needle ablations).
  - Microbenchmarks vs end-to-end latency (InnerQ reports matmul speedups; broader serving impact not fully shown).

### Theme: Evaluation reliability, auditing, and hidden behaviors

- **Why it matters**: Safety and capability claims depend on measurement; rater effects and disagreement ceilings can invert rankings, while auditing tools must be tested against models that actively resist disclosure.
- **Representative papers**:
  - [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755v1)
  - [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585v1)
  - [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758v1)
- **Common approach**:
  - Treat evaluation as a **measurement problem** (MFRM severity/thresholds; variance decomposition; agentic auditing success).
  - Stress-test with **adversarial targets** (implanted hidden behaviors + anti-confession training).
  - Report **diagnostics**, not just scores (rater centrality; tool-to-agent gap; residual disagreement dominance).
- **Open questions / failure modes**:
  - Tool-to-agent gap: evidence surfaced by tools may not translate to investigator success.
  - Identification/estimability constraints in rater models (policy facet not estimable in MFRM attempt).
  - Large residual disagreement suggests limits to “judge model” improvements without better rubrics/context.

### Theme: Privacy & dual-use risk in the agent era

- **Why it matters**: Agents plus tools/memory can amplify privacy harms (deanonymization) and dual-use capability uplift; defenses must be evaluated under realistic, long-horizon human use.
- **Representative papers**:
  - [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079v1)
  - [Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling](https://arxiv.org/abs/2602.23262v1)
  - [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329v1)
- **Common approach**:
  - End-to-end pipelines with **search + aggregation + reflection** (stylometry agent; uplift study with multi-model access).
  - Formal privacy via **DP + post-processing** (DP only on coarse wavelet tokens; public prior for details).
  - Measure not just accuracy but **operational risk signals** (candidate coverage; mitigation via guided recomposition; participant-reported safeguard friction).
- **Open questions / failure modes**:
  - Open-world deanonymization remains low even with DB augmentation (top-3 still modest), but targeted settings improve sharply.
  - DP quality gaps persist at strict ε (e.g., ε=1 artifacts; sensitivity to public prior strength).
  - Translating in-silico uplift to wet-lab risk remains unresolved.

### 3) Technical synthesis
- **GRPO shows up as a unifying optimization primitive** across: adaptive thinking (CPAS/LAGR), agentic RAG (Search-P1), industrial faithfulness RL (Advertising QA), and human-collaboration tool-use (AHCE HFM).
- **A recurring stabilization pattern**: when trajectories vary wildly in length/structure, methods add *explicit normalization/weighting* (LAGR length weights; CPAS advantage offsets; path-centric rewards; difficulty-aware entropy).
- **“Boundary-centric” agent safety is converging**: AgentSentry defends at tool-return boundaries; IoT edge paper highlights MQTT as the command boundary; CourtGuard grounds judgments in retrieved policy text rather than parametric “intuition.”
- **Retrieval is being treated as a policy-learning problem, not a fixed module**: Search-P1 shapes rewards around plan execution and reference step coverage; industrial GraphRAG co-adapts retrieval and generation with RL.
- **Long-horizon reliability is being operationalized with new metrics**: stochasticity via normalized total variance over answers/findings/citations; memory via recall/causal/state-update/abstraction categories; systems security via actuation-to-audit delay and failover blackout windows.
- **Model-driven systems optimization is expanding beyond “better prompts”**: SideQuest uses the model to garbage-collect KV cache; InnerQ aligns quantization grouping with decode-time vector–matrix access patterns.
- **Evaluation is moving toward “measurement models”**: IRT/MFRM adjusts for rater severity/centrality; HealthBench disagreement decomposition shows residual dominates; AuditBench measures end-to-end investigator success rather than tool signal alone.
- **Safety transfer is increasingly parameter- or inference-time**: sparse weight editing for multilingual safety; CourtGuard policy swapping; AgentSentry inference-only counterfactual purification—reducing dependence on large new datasets.
- **Benchmarks are becoming more agent-realistic**: AMA-Bench uses action–observation logs with symbolic artifacts; OmniGAIA requires omni-modal tool use; General Agent Evaluation focuses on protocol-preserving cross-environment comparisons.

### 4) Top 5 papers (with “why now”)

1) [AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification](https://arxiv.org/abs/2602.22724v1)
- Introduces boundary-anchored counterfactual re-executions (orig/mask/mask_sanitized/orig_sanitized) to estimate causal takeover (ACE/IE/DE).
- Reports **ASR = 0%** across three IPI families and multiple black-box models on AgentDojo, with reported FPR = 0% in tables.
- “Why now”: tool-augmented agents are shipping; this is a concrete inference-time layer that aims to *continue safely* rather than terminate.
- **Skepticism**: overhead scales with re-executions per boundary; evaluation notes benchmarks may under-represent long-horizon delayed takeovers.

2) [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755v1)
- Provides **56 target models / 14 hidden behaviors** with anti-confession training, enabling systematic auditing evaluation.
- Finds scaffolded black-box tools outperform many white-box tools overall; documents a **tool-to-agent gap**.
- “Why now”: auditing is becoming a deployment gate; this gives repeatable targets and end-to-end agent evaluation.
- **Skepticism**: targets are fine-tunes on one base model (Llama 3.3 70B); may be easier to audit than organically emergent behaviors.

3) [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329v1)
- Human-subject evidence: LLM access yields **4.16× higher novice accuracy** (odds ratio) vs internet-only; Treatment improves on 7/8 benchmarks.
- Treatment sometimes exceeds expert baselines (e.g., HPCT, VCT) and participants often report little safeguard friction (89.6%).
- “Why now”: policy discussions need uplift data under realistic multi-model, long-duration use—not just model-only benchmarks.
- **Skepticism**: confined to in-silico tasks; model availability changed mid-study; not double-blind.

4) [SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2602.22603v1)
- Uses a **parallel auxiliary thread** to decide which tool outputs are stale and delete their KV entries without polluting main context.
- Reports large efficiency gains (peak token utilization −56–65%, KV reads −53–71%) and serving throughput +83.9% on H100 for FRAMES.
- “Why now”: deep-research/web agents are KV-bound; this is a practical serving-side lever.
- **Skepticism**: eviction limited to tool outputs (not “thoughts”); some OOD accuracy degradation (BrowseComp).

5) [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554v1)
- Training-free sparse neuron editing with a closed-form low-rank update to transfer English safety behavior to other languages.
- Introduces MULTI-STRONGREJECT (8 languages, 313 harmful prompts each) and shows unsafe-count reductions across models; composes with MPO.
- “Why now”: multilingual jailbreak gaps are a real deployment vulnerability; weight editing is fast to iterate and deploy.
- **Skepticism**: evaluation relies on an automated guard model; datasets are machine-translated (may miss natural LRL jailbreaks).

### 5) Practical next steps
- **Add boundary instrumentation to agents**: log tool-return boundaries with provenance metadata and run periodic “shadow” counterfactual checks (AgentSentry-style) on high-risk tools/actions.
- **Treat messaging middleware as part of the safety perimeter** in edge/IoT: enforce MQTT authentication/ACLs and replay protection; measure actuation-to-audit delay and failover blackout windows as first-class safety SLOs.
- **If doing agentic RAG RL**, try path-centric rewards (self-consistency + reference step coverage) and soft outcome scoring; explicitly test evaluator sensitivity by swapping judge models.
- **Reduce long-horizon cost without breaking correctness**: implement adaptive thinking control tokens and stabilize RL with length-aware gradient regulation; separately test difficulty-aware entropy regularization to prevent entropy collapse.
- **Make reliability measurable** for research agents: compute run-to-run variance over answers/findings/citations; then apply structured outputs + early query intersection ensembling to reduce stochasticity while tracking accuracy.
- **For multilingual deployments**, run a multilingual harmful-prompt sweep and consider sparse weight edits as a fast patch—while validating with multiple harm classifiers (not just one guard).
- **Upgrade human evaluation pipelines**: model rater severity/centrality (MFRM) and track disagreement decomposition; prioritize collecting “reducible uncertainty” tags or missing-context annotations where disagreement is high.
- **For auditing programs**, evaluate tools end-to-end with an investigator agent (AuditBench-style), not just tool signal; explicitly test hardest target configurations (e.g., TD+KTO) to avoid overfitting to easy-to-audit organisms.

---
*Generated from per-paper analyses; no external browsing.*

