---
layout: single
title: "Daily AI Paper Report (2026-02-27)"
date: 2026-02-27
permalink: /paper-news/2026-02-27/
author_profile: true
---


> Chinese version: [[中文]](/paper-news/2026-02-27-zh/)

# AI Paper Insight Brief
## 2026-02-27

### 0) Executive takeaways (read this first)
- **Agent safety is shifting from “prompt-level” to “systems-level”**: edge/IoT deployments inherit insecure coordination planes (e.g., MQTT accepting spoof/replay) and create *multi-second audit blind spots* during failover—risks that won’t be caught by model-only guardrails.
- **Inference-time, training-free safety adaptation is maturing** in two directions: (i) *policy-grounded adjudication* (CourtGuard) for rapid policy swaps, and (ii) *causal counterfactual diagnostics* (AgentSentry) that can drive attack success to **0% ASR** on AgentDojo while preserving utility.
- **GRPO is becoming the default RL backbone**, but the frontier is now *stability/efficiency shaping*: adaptive thinking (CPAS+LAGR) and difficulty-aware entropy regularization (CEEH) both target length/entropy pathologies; agentic RAG (Search-P1) targets reward sparsity with path-centric process rewards.
- **Long-horizon agent performance is increasingly bottlenecked by infrastructure, not raw model IQ**: KV-cache management (SideQuest) and KV quantization layout (InnerQ) show large throughput/latency wins; memory is now benchmarked in agent-native trajectories (AMA-Bench), not just dialogue/doc QA.
- **Evaluation methodology is under active repair**: rater-effect modeling (MFRM) can flip system rankings in human eval; physician disagreement in HealthBench is mostly case-specific residual; deep research agents show high run-to-run variance that can be reduced with structured outputs + query ensembling.
- **Biosecurity risk evidence is becoming more direct**: a human uplift study finds LLM access makes novices **4.16× more accurate** than internet-only on in silico biology tasks, and most participants report little difficulty overcoming safeguards.

### 2) Key themes (clusters)

### Theme: Systems security for tool-using agents (edge + prompt injection)

- **Why it matters**: As agents move into physical/edge settings and long-horizon tool loops, the dominant failures become *coordination-plane integrity*, *audit latency*, and *trust-boundary violations*—not just prompt injection in a single chat turn.
- **Representative papers**:
  - Systems-Level Attack Surface of Edge Agent Deployments on IoT — https://arxiv.org/abs/2602.22525v1
  - AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification — https://arxiv.org/abs/2602.22724v1
- **Common approach**:
  - Treat agent safety as **boundary management** (tool-return boundaries; MQTT as C2 plane).
  - Use **measurable systems metrics** (actuation-to-audit delay, provenance completeness, failover windows).
  - Add **inference-time security layers** (counterfactual “shadow runs”, context purification) without retraining.
- **Open questions / failure modes**:
  - How to harden coordination planes (e.g., MQTT) with enforceable provenance/ACLs without breaking latency/availability.
  - Counterfactual defenses add overhead; unclear behavior on **long-horizon progressive takeovers** (benchmark gap noted).
  - Failover can create **tens-of-seconds monitoring gaps**; how to guarantee safe actuation during blackout windows.

### Theme: Inference-time alignment & policy agility (no retraining)

- **Why it matters**: Deployed systems face shifting policies (ToS, compliance regimes) and multilingual jailbreak surfaces; retraining-based alignment lags operational needs.
- **Representative papers**:
  - CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety — https://arxiv.org/abs/2602.22557v1
  - Multilingual Safety Alignment Via Sparse Weight Editing — https://arxiv.org/abs/2602.22554v1
- **Common approach**:
  - **Policy-grounded RAG** + structured adjudication (debate + judge with threat scores).
  - **Training-free parameter edits**: sparse neuron selection + closed-form low-rank updates to transfer safety across languages.
  - Compose with existing methods (e.g., MPO + editing).
- **Open questions / failure modes**:
  - CourtGuard latency and formatting brittleness on smaller backbones.
  - Multilingual benchmark/eval relies on **machine translation** and an automated guard model; robustness to adaptive attacks not established.

### Theme: RL for efficiency & stability in reasoning / agentic RAG

- **Why it matters**: “More thinking” is no longer a free win; token costs and instability (entropy collapse, length heterogeneity, sparse rewards) are now first-order constraints.
- **Representative papers**:
  - Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation — https://arxiv.org/abs/2602.22556v1
  - Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning — https://arxiv.org/abs/2602.22642v1
  - Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training — https://arxiv.org/abs/2602.22576v1
- **Common approach**:
  - GRPO-style RL with **explicit length/efficiency objectives**.
  - Fix known RL pathologies: group-normalized advantage bias (CPAS), gradient imbalance from length heterogeneity (LAGR), entropy collapse (difficulty-aware entropy).
  - Densify supervision for agentic RAG via **process/path rewards** and partial credit.
- **Open questions / failure modes**:
  - Generalization beyond math/verifiable domains remains limited (explicitly noted for adaptive thinking).
  - Reliance on LLM evaluators/reference planners (Search-P1) introduces sensitivity to evaluator choice and offline cost.

### Theme: Long-horizon agent scaling: memory, KV cache, and context management

- **Why it matters**: Long-horizon agents are increasingly constrained by **GPU memory bandwidth and context growth**, and by memory systems that fail on structured action–observation logs.
- **Representative papers**:
  - SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning — https://arxiv.org/abs/2602.22603v1
  - InnerQ: Hardware-aware Tuning-free Quantization of KV Cache for Large Language Models — https://arxiv.org/abs/2602.23200v1
  - AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications — https://arxiv.org/abs/2602.22769v1
- **Common approach**:
  - Treat tool outputs as first-class context objects and **evict/quantize** them aggressively.
  - Align compression with hardware primitives (vector–matrix decode attention) and serving stacks (SGLang/H100 evals).
  - Benchmark memory on **agent-environment trajectories** with causal/state questions (Recall, Causal Inference, State Updating, Abstraction).
- **Open questions / failure modes**:
  - SideQuest currently evicts only tool responses (not “thought” tokens) and shows some OOD degradation.
  - InnerQ evidence is limited to GSM8K + matmul microbench on one GPU; end-to-end decode gains across workloads remain to be shown.
  - Memory construction/compression is a major loss point (needle ablations show large drops).

### Theme: Evaluation reliability: rater effects, disagreement ceilings, and agent stochasticity

- **Why it matters**: If measurement is unstable, optimization targets drift; for agents, *run-to-run variance* can dominate perceived quality.
- **Representative papers**:
  - Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach — https://arxiv.org/abs/2602.22585v1
  - Decomposing Physician Disagreement in HealthBench — https://arxiv.org/abs/2602.22758v1
  - Evaluating Stochasticity in Deep Research Agents — https://arxiv.org/abs/2602.23271v1
- **Common approach**:
  - Explicit variance decomposition (rater severity/centrality; physician vs rubric vs residual; propagated vs intrinsic stochasticity).
  - Diagnose ceilings and propose mitigation levers (rater monitoring; uncertainty tags; structured outputs + query ensembling).
- **Open questions / failure modes**:
  - Many-facet models need sufficient linkage/overlap; some facets not estimable in practice.
  - HealthBench disagreement is largely residual/case-specific; many proposed predictors explain little variance.
  - API non-determinism persists even at temperature 0; mitigation must be algorithmic.

### Theme: Auditing & red-teaming hidden behaviors and relational harms

- **Why it matters**: Hidden behaviors and multi-turn harms are hard to surface with static tests; agentic auditors and adversarial simulations are emerging as scalable alternatives.
- **Representative papers**:
  - AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors — https://arxiv.org/abs/2602.22755v1
  - TherapyProbe: Generating Design Knowledge for Relational Safety in Mental Health Chatbots Through Adversarial Simulation — https://arxiv.org/abs/2602.22775v1
- **Common approach**:
  - Build **model organisms** with concealed behaviors and evaluate end-to-end investigator agents (tool-to-agent gap).
  - Use adversarial multi-agent simulation (MCTS) to discover **multi-turn failure paths** and abstract them into pattern libraries.
- **Open questions / failure modes**:
  - AuditBench targets are fine-tunes; may be easier to audit than organically emergent issues.
  - TherapyProbe detector performance is bounded (macro-F1 0.71; some categories harder), and practitioner validation is small.

### 3) Technical synthesis
- GRPO appears repeatedly as the RL workhorse (adaptive thinking, human-collaboration module, agentic RAG reward shaping, industrial RAG RL), but papers increasingly focus on **variance/heterogeneity control** (length-aware gradients, entropy collapse prevention, partial-credit outcomes).
- A shared pattern across agent training papers: **replace sparse terminal rewards with structured intermediate signals** (Search-P1 path reward; diffusion step scoring via PRM; AgentDropoutV2 indicator-triggered verification).
- “Inference-time scaffolding” is converging: CourtGuard (RAG+debate), AgentSentry (counterfactual shadow runs), AgentDropoutV2 (rectify-or-reject), and SideQuest (parallel auxiliary thread) all add **auxiliary decision loops** around a base model rather than changing core weights.
- Multiple works highlight that **tool outputs dominate long-horizon context** and are the natural unit for compression/eviction (SideQuest) and for security boundaries (AgentSentry).
- Evaluation papers collectively suggest a new default: report not just mean accuracy, but **measurement stability** (rater-adjusted latent traits; disagreement decomposition; run-to-run variance TV for agents).
- The “policy layer” is being externalized: CourtGuard grounds decisions in retrieved governance text; ESAA enforces JSON contracts + deterministic replay; IoT edge paper proposes provenance completeness and audit delays as first-class metrics.
- Benchmarks are becoming more *agent-native*: AMA-Bench uses action–observation trajectories; OmniGAIA requires multimodal tool use; AuditBench evaluates an investigator agent, not just a classifier.
- Several papers show that **system constraints can hard-fail capability** (General Agent Evaluation: tool-count limits causing 0.00 on AppWorld; IoT failover windows; KV cache memory bandwidth).
- Safety transfer is being attacked at two ends: **parameter edits** for multilingual safety (sparse weight editing) and **runtime adjudication** for policy swaps (CourtGuard), suggesting a hybrid stack may be practical.

### 4) Top 5 papers (with “why now”)

1) AgentSentry: Mitigating Indirect Prompt Injection in LLM Agents via Temporal Causal Diagnostics and Context Purification — https://arxiv.org/abs/2602.22724v1
- Introduces boundary-level counterfactual re-executions (orig/mask/mask_sanitized/orig_sanitized) to estimate causal takeover signals (ACE/IE/DE).
- Reports **ASR = 0%** across three IPI families and multiple black-box models on AgentDojo, with high utility and **FPR = 0%** in reported tables.
- Ablations show sanitized counterfactuals + masking probe are critical (removing them raises ASR to ~21–23%).
- **Skepticism**: added inference overhead; benchmark may under-represent long-horizon progressive/delayed takeovers.

2) LLM Novice Uplift on Dual-Use, In Silico Biology Tasks — https://arxiv.org/abs/2602.23329v1
- Human-subject evidence: LLM access makes novices **4.16× more accurate** than internet-only (adjusted accuracy ~5% → >17%).
- Treatment beats control on **7/8** benchmarks; sometimes exceeds expert baselines (e.g., HPCT, VCT).
- Reports most participants (89.6%) indicated **no difficulty overcoming safeguards**, relevant for policy and deployment.
- **Skepticism**: limited to in silico tasks; not double-blind; model availability changed mid-study.

3) SideQuest: Model-Driven KV Cache Management for Long-Horizon Agentic Reasoning — https://arxiv.org/abs/2602.22603v1
- Turns KV eviction into a learned auxiliary task in a **parallel thread**, avoiding “management token” pollution.
- Large efficiency gains: **56–65%** peak token utilization reduction; serving eval shows **+83.9% throughput** on H100 for FRAMES.
- Improves reliability vs heuristic eviction baselines (fewer unparsable/non-completion failures).
- **Skepticism**: limited fine-tuning data (215 traces) and eviction only for tool outputs; some OOD accuracy loss.

4) CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety — https://arxiv.org/abs/2602.22557v1
- Practical “policy swap” safety: RAG-grounded attacker/defender debate + judge outputs SAFE/BORDERLINE/UNSAFE with threat scores.
- Strong reported benchmark performance (macro avg Acc 0.87 / F1 0.86) and **90%** accuracy on OOD Wikipedia vandalism via policy corpus swap.
- Explicitly frames **Dynamic Policy Adaptability** as a desideratum.
- **Skepticism**: higher latency; depends on backbone formatting adherence (parsing errors risk).

5) Systems-Level Attack Surface of Edge Agent Deployments on IoT — https://arxiv.org/abs/2602.22525v1
- Makes edge-agent security concrete with measurable metrics (audit delay, provenance completeness, sovereignty egress, failover windows).
- Empirically shows MQTT broker accepts spoof/replay/malformed provenance; forced fallback triggers silent DNS to external APIs; failover window **35.7s**.
- Highlights that “sovereignty” can fail invisibly under fallback—critical for physical actuation contexts.
- **Skepticism**: single small testbed; no mitigation prototypes; cloud egress comparison not workload-matched.

### 5) Practical next steps
- For tool-using agents, add **tool-return boundary instrumentation**: log mediator content, proposed action, and (if feasible) run lightweight counterfactual checks on high-impact actions (AgentSentry-style).
- If deploying edge/IoT agents, treat MQTT (or equivalent) as **safety-critical C2**: measure provenance completeness, actuation-to-audit delay, and failover blackout windows; test spoof/replay acceptance explicitly.
- For long-horizon agents, prioritize **context cost controls**: prototype tool-output eviction (SideQuest-like) and/or KV-cache quantization aligned to decode matmuls (InnerQ-like), then measure end-to-end throughput and failure rates.
- When training agentic RAG with RL, move beyond binary outcome rewards: implement **path/process rewards** and partial credit (Search-P1) and track convergence speed, not just final accuracy.
- For reasoning efficiency RL, monitor **entropy collapse and length heterogeneity**; consider difficulty-aware entropy regularization (CEEH) or length-aware gradient regulation (LAGR) depending on your failure mode.
- In evaluation pipelines, stop treating raw Likert means as ground truth: fit a **rater-effect model** (MFRM) when feasible, and report disagreement ceilings/variance decompositions (HealthBench-style).
- For deep research agents, measure **run-to-run variance** (answers/findings/citations) and test mitigations like structured outputs and early query ensembling; don’t rely on temperature=0 for determinism.
- For biosecurity governance, incorporate **human uplift** studies (not just model-only benchmarks) into risk assessments, and explicitly test whether safeguards meaningfully slow information acquisition in multi-hour workflows.

---
*Generated from per-paper analyses; no external browsing.*

