---
permalink: /
title: "Welcome to My Homepage"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Biography
-----
I am currently an Assistant Professor at Sun Yat-Sen University. Before that, I was a Postdoctoral Fellow, working with Prof. [XiaoFeng Wang](https://wangxiaofeng7.github.io) and Prof. [Haixu Tang](https://luddy.indiana.edu/contact/profile/?Haixu_Tang). I received my Ph.D. degree with honors in Information Engineering from the Chinese University of Hong Kong, advised by Prof. [Kehuan Zhang](https://staff.ie.cuhk.edu.hk/~khzhang/) and Prof. [Dahua Lin](http://dahua.site/). 



My research is centered on **AI Security**, with a particular emphasis on **Trustworthy ML** and **Responsible AI**. Recently, I have been focusing on exploring the security challenges associated with emerging **Large Generative Models** (e.g., ChatGPT and DeepSeek). In my view, an AI system that lacks trustworthiness---or can be deemed irresponsible---pertains to a system whose functionality deviates from its original design, either due to intentional manipulation or unintentional defects. An adversary may deliberately alter an AI system to generate unreliable results, while unintentional bias within the AI model itself can also lead to outputs that lack trustworthiness. In my research, I alwasy strive to deliver dependable **Security Guarantees** for AI models.


In my existing research, I examined how adversaries could compromise the functionality of an AI system and identified countermeasures against such attacks. Specifically, I discovered carefully crafted patterns on web pages capable of manipulating search engine rankings (such as Google, Bing, etc.) [(CCS'22)](https://dl.acm.org/doi/abs/10.1145/3548606.3560683) (which earned the *Best Paper Honorable Mention Award*). Additionally, I discovered how malicious users on shared cloud platforms can hijack the output of deployed AI systems [(Usenix'24)](https://www.usenix.org/conference/usenixsecurity24/presentation/wang-zihao-tossing), and I proposed a simple yet highly effective method to bypass widely adopted AI backdoor detection techniques [(NDSS'24)](https://www.ndss-symposium.org/ndss-paper/gradient-shaping-enhancing-backdoor-attack-against-reverse-engineering/). From a defense perspective, I introduced the *First* technique using *reflected light* to detect fake faces in front of AI-based face recognition systems, named as **Face Flashing** [(NDSS'18)](https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2018_03B-5_Tang_paper-updated.pdf). This method has since been adopted by WeChat and is now used by billions of people daily [(news)](https://www.ie.cuhk.edu.hk/the-system-security-lab-led-by-prof-zhang-kehuan-designed-face-flashing-protocol-to-increase-precision-of-face-recognition-and-blocks-log-in-attacks/). 
Additionally, I emphasized the latent space’s role in distinguishing benign from malicious inputs into AI models (**SCAn** in [Usenix'21)](https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di)
, and introduced a theoretical framework for evaluating the detection difficulty of various AI backdoors [(paper)](https://arxiv.org/abs/2210.06509). This backdoor-related research helped secure $704,980 in funding for the TrojAI project [(TrojAI)](https://pages.nist.gov/trojai/), where I played a pivotal role, as well as an additional $1,133,213 in follow-up funding. Moreover, the backdoor detection method that I proposed helped me **Win the First Place** in the Trojan Detection Competition 2022, hosted by NeurIPS [(homepage)](https://2022.trojandetection.ai/index).
In addition to the general trustworthiness of AI, I also enhanced the reliability of specific AI tools for particular tasks. I improved the robustness of an AI for identifying the root cause of bugs across diverse code bases [(Usenix'24)](https://www.usenix.org/conference/usenixsecurity24/presentation/xu-dandan), reduced the false positives of an AI model that is designed to detect logic flaws in 5G documentation [(Usenix'22)](https://www.usenix.org/conference/usenixsecurity22/presentation/chen-yi), and ensured that the AI tool for conformance testing covered all scenarios [(Usenix'23)](https://www.usenix.org/conference/usenixsecurity23/presentation/chen-yi).



For Potential Students and Collaborators
-----
I am always seeking highly **motivated students** who are passionate about advancing research in AI Security. I firmly believe that groundbreaking ideas are born through collaboration, and I am thrilled to partner with researchers who are driven to push the boundaries of knowledge and create meaningful, real-world impact. If you are enthusiastic, intellectually curious, and committed to achieving excellence, I would love to explore opportunities to work together and deliver impactful, cutting-edge results. **Let’s join forces to tackle the most pressing challenges in AI Security and shape the future of trustworthy and responsible AI!**<br>
Drop me an [Email](mailto:tangd9@mail.sysu.edu.cn) if you are interested!




News
------
* 01/2025: I have joined Sun Yat-Sen University as an Assistant Professor.